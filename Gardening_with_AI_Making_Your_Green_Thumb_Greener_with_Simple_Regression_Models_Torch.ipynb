{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3zcYixfUmWpR90OxN3LpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hdkhosravian/deep_learning_tutorial/blob/main/Gardening_with_AI_Making_Your_Green_Thumb_Greener_with_Simple_Regression_Models_Torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**\n",
        "\n",
        "In our case, think of \"Data Preparation\" as preparing the soil and seeds for your garden. The soil and seeds are the basis for your garden, just as data is the basis for our model.\n",
        "\n",
        "We're creating a hypothetical garden where each plant is influenced by factors like Growing Days, Soil pH, Sunlight Hours, and Humidity. The plants, in our scenario, represent our data points, and these factors are akin to the different nurturing elements necessary for growth.\n",
        "\n",
        "The \"Plant Height\" we're aiming to predict is the outcome of these combined factors, much like the height of a plant in your garden is the outcome of how many days it's been growing, the pH of the soil it's planted in, how many hours of sunlight it receives, and the level of humidity it's exposed to.\n",
        "\n",
        "Let's create our garden, or in other words, let's generate our dataset:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Number of plants (data points)\n",
        "n = 1000\n",
        "\n",
        "# Create the dataframe (garden)\n",
        "df = pd.DataFrame({\n",
        " 'Growing Days': np.random.randint(20, 50, n),\n",
        " 'Soil pH': np.random.uniform(6.0, 7.5, n).round(1),\n",
        " 'Sunlight Hours': np.random.uniform(5.0, 9.0, n).round(1),\n",
        " 'Humidity': np.random.randint(50, 80, n),\n",
        " 'Plant Height': np.random.randint(20, 50, n)\n",
        "})\n",
        "\n",
        "df.head()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sLppDbkZenQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a pandas dataframe with n (1000) entries. Each entry represents a plant with its corresponding characteristics (Growing Days, Soil pH, Sunlight Hours, Humidity) and outcome (Plant Height). We use the np.random functions to simulate variability in these factors and outcomes, just like in a real garden where these factors vary from plant to plant.\n",
        "\n",
        "At the end of this step, we have our garden ready, and each plant's data can be used for the next steps of our AI-assisted gardening journey!"
      ],
      "metadata": {
        "id": "c6VMQ3C_e0fZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train-Test Split**\n",
        "\n",
        "The Train-Test split is like an exam preparation strategy for our AI model, similar to how students prepare for their exams.\n",
        "\n",
        "Think of the entire dataset as the complete syllabus for the exam. Now, students don't go to the exam hall without studying and revising, right? They read their textbooks, take notes, and learn the material first - this is their \"training\". And, they don't use the entire syllabus for revision. Instead, they leave some questions or chapters for self-testing and assessment - this is their \"testing\" or validation phase.\n",
        "\n",
        "That's exactly what we are doing here with our data. We split our data into a training set (around 80% of the data) and a testing set (the remaining 20%). The training set is like the textbook. Our model learns from this data - it adjusts its weights and biases based on the features and their corresponding target values in the training set.\n",
        "\n",
        "Once the model has been trained, we need to assess how well it has learned. We do this by using the testing set. The model has never seen this data during the training process. So, this phase is like the exam - we're testing the model's ability to make correct predictions on new, unseen data.\n",
        "\n",
        "Let's see this in action:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Importing the necessary library\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into input features (X) and target variable (y)\n",
        "X = df.drop('Plant Height', axis=1).values\n",
        "y = df['Plant Height'].values\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fnpntQRKe53H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we first separate our features (X) and target (y). We then use the train_test_split function from the sklearn.model_selection module to divide our data into training and testing sets. The test_size=0.2 argument means that 20% of the data will be set aside for testing, and the remaining 80% will be used for training. The random_state parameter ensures reproducibility - with the same random state, you'll get the same train-test split every time you run the code.\n",
        "\n",
        "By the end of this step, we have our \"textbook\" (X_train, y_train) and our \"exam\" (X_test, y_test) ready for the next steps!\n"
      ],
      "metadata": {
        "id": "GZkX3ca-fW0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization in data preparation is a lot like converting various currencies to a single, universal one when dealing with international trade.\n",
        "\n",
        "Imagine you own an international business that involves transactions in different currencies such as the US Dollar, Euro, British Pound, and Japanese Yen. When analyzing the financial data of your business, dealing with multiple currencies could get confusing and cumbersome. Moreover, each of these currencies has a different scale: 1 US Dollar is not the same as 1 Euro, or 1 British Pound, or 1 Japanese Yen. The solution? Convert all transactions to a common currency, say, US Dollar. Now, you have standardized the financial data, making it simpler and more efficient to analyze.\n",
        "\n",
        "Similarly, our AI model can face difficulty when the features (input variables) are on different scales. For instance, 'Growing Days' might range from 20 to 50, 'Sunlight Hours' from 5.0 to 9.0, and 'Humidity' from 50 to 80. Each of these features is on a different scale, like different currencies. If we feed these features to our model as they are, the model might get confused and give undue importance to features with larger scales.\n",
        "\n",
        "Standardizing these features means that we convert all features to the same scale, much like converting all currencies to one standard currency. It involves subtracting the mean and dividing by the standard deviation of each feature, bringing them all to a standard scale where the mean is 0 and the standard deviation is 1. This makes it easier for our model to learn from the data.\n",
        "\n",
        "Here's how we do it in Python:\n",
        "\n",
        "\n",
        "```\n",
        "# Importing the necessary library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize a scaler using StandardScaler\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "\n",
        "# Apply the scaler to the training and testing data\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "FJjHtSrxffnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize a StandardScaler and use the fit method on the training data (X_train). This calculates the mean and standard deviation of each feature in the training data. We then transform our training data using these calculated values with the transform method. The mean of each feature in the transformed data is now 0 and the standard deviation is 1.\n",
        "\n",
        "We also apply this transformation to the testing data (X_test) using the same scaler. We use the same scaler to ensure that the model sees the testing data in the same way as the training data. If we fit a new scaler to the testing data, it would have different mean and standard deviation values, creating inconsistency between the training and testing sets.\n",
        "\n",
        "Remember, we always fit the scaler to the training data only, not the testing data. The testing data is supposed to be unseen, new data, so we shouldn't use it to influence any part of the training process.\n",
        "\n",
        "After this step, our data is ready to be fed into the neural network. All features are now speaking the same \"language\", making it easier for the model to learn."
      ],
      "metadata": {
        "id": "1TvRmRAXf_9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Architecture**\n",
        "\n",
        "Creating our model architecture is like designing the blueprint of a building. Each component of the building has a specific role and is connected in a particular way to make the whole structure functional. Similarly, in a neural network, we have different layers (floors of the building), and each layer has neurons (rooms on the floor) performing specific operations.\n",
        "\n",
        "In our plant height prediction project, we need to design a model that can accept our garden data, process it through a series of operations, and finally make a prediction. We use a specific type of neural network for this, called a fully connected or dense network, where every neuron in each layer is connected to every neuron in the next layer.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Importing necessary library\n",
        "from torch import nn\n",
        "\n",
        "# Define a PyTorch model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "cSXxGGh6g3xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `class Net(nn.Module):` - Just as a garden has a plan, our machine learning model also needs a plan or a blueprint. Here, we're defining that blueprint and calling it \"Net\". It is a subclass of PyTorch's nn.Module, which means it will inherit all the properties and functions of a general PyTorch model.\n",
        "\n",
        "2. `def __init__(self, input_size, hidden_size, output_size):` - This is our model's constructor. It's like defining what type of plants we're going to grow (input_size), how much we're going to care for them (hidden_size), and how tall we expect them to grow (output_size).\n",
        "\n",
        "3. `super(Net, self).__init__():` - With this line, we're calling the constructor of nn.Module, the parent class. It's like referring back to general gardening knowledge before we apply our specific plan.\n",
        "\n",
        "4. `self.layer1 = nn.Linear(input_size, hidden_size)` - Here, we're defining our first layer of the neural network. Think of it as the first stage of growing a plant where we plant seeds (input) and start nurturing them (hidden nodes). `nn.Linear` means that every input is connected to every hidden node, much like each seed receiving uniform care and attention.\n",
        "\n",
        "5. `self.layer2 = nn.Linear(hidden_size, output_size)` - This is the second stage of plant growth where our nurtured plants (hidden nodes) grow to their full height (output). Again, `nn.Linear` ensures that every nurtured plant contributes to the final output.\n",
        "\n",
        "6. `def forward(self, x):` - This is our model's growth process. We take our initial conditions (`x`, or seeds), and put them through the two stages we defined (layer1 and layer2).\n",
        "\n",
        "7. `x = torch.relu(self.layer1(x))` - Here, our seeds go through the first stage of growth. The `torch.relu` is an activation function, acting like a growth regulation factor that controls how much each input (seed) contributes to the hidden nodes (nurturing stage).\n",
        "\n",
        "8. `x = self.layer2(x)` - Now, our nurtured plants go through the second stage and grow to their final height. The output here (`x`) is our final plant height.\n",
        "\n",
        "9. `return x` - We're returning the final plant heights predicted by our model."
      ],
      "metadata": {
        "id": "nJYQK0Ulhn_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **why do we need activation functions?**\n",
        "\n",
        "They introduce non-linearity to our model. In real life, things are not usually linear. For example, doubling the amount of water you give to a plant won't necessarily double its growth. Activation functions help our model capture these non-linear relationships, making its predictions more accurate and versatile."
      ],
      "metadata": {
        "id": "jEvaZ_bi0dp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss Function and Optimizer Definition**\n",
        "Defining the loss function and the optimizer in a machine learning model is akin to setting up a system for learning from mistakes and making improvements, much like a gardener trying different methods to optimize plant growth.\n",
        "\n",
        "Let's look at the loss function first:\n",
        "\n",
        "\n",
        "```\n",
        "criterion = nn.MSELoss()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xOVAUuIc1Cw_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this line, we're defining the loss function to be Mean Squared Error (MSE). Think of it as a gardener measuring the difference between the actual height of the plant and the height he expected the plant to reach. If the plant is shorter than expected, there's a 'loss' or error. If the plant is taller, that's also a deviation. We square these deviations (to make sure they are positive) and take the mean of all squared errors. This gives us a single number that tells us how off our predictions are from the actual values, or in the gardener's context, how off his expectation was from the actual plant height.\n",
        "\n",
        "\n",
        "Next, let's look at the optimizer:\n",
        "\n",
        "\n",
        "```\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EDjJm6uLyaag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Think of SGD as a very determined, yet slightly short-sighted gardener, who is trying to climb to the top of a hill (our optimal model), but can only see a few feet ahead (current batch of data). The gardener's goal is to reach the top of the hill (the best model that minimizes error) as quickly as possible.\n",
        "\n",
        "Step Size (Learning Rate): The size of the steps the gardener takes is like our learning rate (lr). If the steps are too large (lr is too high), the gardener might overshoot the top and end up on the other side of the hill. If the steps are too small (lr is too low), it could take the gardener a very long time to reach the top. So, the learning rate is a crucial aspect of SGD that needs to be set correctly.\n",
        "In our code, lr=0.01 means the gardener is taking small steps towards the top of the hill.\n",
        "\n",
        "\n",
        "1.   Stochastic (Random): The word 'stochastic' means random. When the gardener can't see the entire hill, he decides to move in the direction which seems to be the steepest, hoping it will lead him to the top. In the case of our model, instead of looking at the entire dataset (the whole hill), we look at a small random sample (current surroundings of the gardener), calculate the error (determine the slope), and make a step (update the model parameters).\n",
        "\n",
        "2.   Gradient Descent: The word 'gradient' refers to the slope, and 'descent' means to go down. But wait, aren't we trying to climb the hill, not descend? The confusion arises because we're technically trying to minimize the error (descend a hill of error), even though we often visualize this as trying to maximize accuracy (climb a hill of accuracy). So, the gardener is using the steepness of the slope to guide his steps.\n",
        "\n",
        "\n",
        "Overall, Stochastic Gradient Descent is a strategy where the model, like a gardener, is trying to find the best parameters (location on the hill) that minimize the error (distance from the top), by taking steps proportional to the negative of the gradient (opposite of the slope) at the current point, based on a small random sample of the total data (limited view of the hill). And it keeps repeating this process, improving with each step, until it reaches the best possible point (the top of the hill)."
      ],
      "metadata": {
        "id": "FG2YG3USybIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  \n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()  \n",
        "    optimizer.step()\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. `for epoch in range(epochs):` - The term 'epoch' is like one complete cycle of a gardener trying different gardening methods on all of his plants in the garden and learning from it. The more epochs, the more times the gardener tries different methods and learns from the plants' reactions. The range of epochs is the number of times we want the model (our gardener) to learn from the entire dataset (all plants in the garden).\n",
        "\n",
        "2. `model.train()` - Here, we're simply telling our model that it's training time. It's like the gardener getting his tools and plants ready for a new cycle of gardening.\n",
        "\n",
        "3. `optimizer.zero_grad()` - Before calculating new gradients, it's essential to set the old ones to zero. If we don't zero the gradients, they will accumulate and interfere with the current gradients. It's like erasing the blackboard before writing new information; otherwise, it would be tough to read and understand what's written.\n",
        "\n",
        "4. `outputs = model(inputs)` - In this line, the model is making a prediction (outputs) based on the current input data (inputs). It's like the gardener predicting how tall a plant will grow given the current soil pH, sunlight hours, and other factors.\n",
        "\n",
        "5. `loss = criterion(outputs, targets)` - Here, we're calculating the 'loss' or error between the model's predictions (outputs) and the actual plant heights (targets). This tells the gardener how far off his predictions were from the actual results.\n",
        "\n",
        "6. `loss.backward()` - This line is where the model starts learning from its mistakes. It calculates the gradients of the loss function with respect to the model parameters. Gradients can be thought of as the directions and magnitudes to adjust the parameters. Without this step, the model wouldn't know how to improve its predictions.\n",
        "\n",
        "7. `optimizer.step()` - Now, the optimizer uses the gradients to update the model parameters. This is like a step of learning or an adjustment made based on the mistakes. The model parameters are tweaked slightly in the direction that reduces the loss. Without this step, the model wouldn't learn from its mistakes, and the loss wouldn't decrease over time.\n",
        "\n",
        "8. `print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))` - This line just prints out the progress of training so we can monitor it. The gardener is simply making a note of how well his current gardening methods are working.\n",
        "\n",
        "In a nutshell, during model training, the model, like a gardener, is learning from its mistakes and improving its predictions (gardening methods) iteratively until it can predict the plant heights as accurately as possible. This is why we have to go through this process: without it, our model wouldn't learn from the data, and wouldn't be able to make accurate predictions.\n"
      ],
      "metadata": {
        "id": "JR8kUbas2JVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**\n",
        "\n",
        "\n",
        "```\n",
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    predictions = model(test_inputs)\n",
        "    predictions = torch.flatten(predictions)\n",
        "\n",
        "test_loss = criterion(predictions, test_targets)\n",
        "print(f'Test Loss: {test_loss.item()}')\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fOFFsQQ4umex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `model.eval()` - First, we tell our model to switch into evaluation mode with `model.eval()`. This is like telling our gardener, \"Now is the time to test your knowledge and predict the height of new plants. Don't learn anything new for now.\"\n",
        "\n",
        "2. `with torch.no_grad():` - This line of code is us asking PyTorch to perform the next operations without tracking gradients. This is because during testing we don't want to update our weights and biases, we just want to test our model. It's like telling our gardener not to change his methods based on these new plants.\n",
        "\n",
        "3. `predictions = model(test_inputs)` - Here, we ask our model to predict the output (`predictions`) given our test inputs. It's like the gardener predicting the height of new plants based on what he has learned.\n",
        "\n",
        "4. `predictions = torch.flatten(predictions)` - Sometimes, our predictions come out in a different shape than we want (for example, they might be in a two-dimensional matrix but we want a one-dimensional vector). To make sure our predictions and actual targets are in the same shape, we 'flatten' our predictions. It's like our gardener listing down his predictions for each plant height one after another in a single line.\n",
        "\n",
        "5. `test_loss = criterion(predictions, test_targets)` - Here, we're calculating our test loss, which is the error of our model on the test data. We use the same criterion we used for training. This tells us how far off our predictions were from the actual results. It's like comparing the gardener's predicted plant heights to their actual heights and seeing how much he was off by.\n",
        "\n",
        "6. `print(f'Test Loss: {test_loss.item()}')` - Finally, we print out the test loss. This gives us a numerical value of how well (or not so well) our model performed. It's like telling the gardener: \"On average, you were off by this much in your predictions.\"\n",
        "\n",
        "Each step here is necessary for testing our model effectively. We need to put our model in evaluation mode, prevent it from learning from the test data, make predictions, ensure those predictions are in the correct shape, calculate the loss to quantify the error, and then print it out to know how our model performed. This testing phase is crucial to verify that our model has indeed learned meaningful patterns from the data and is not just memorizing it, and that it can generalize its learning to new, unseen data."
      ],
      "metadata": {
        "id": "c0YIMJ5CupX7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FULL CODE**"
      ],
      "metadata": {
        "id": "Obss9FFdvt-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Number of plants (data points)\n",
        "n = 10000\n",
        "\n",
        "# Create the dataframe (garden)\n",
        "df = pd.DataFrame({\n",
        "    'Growing Days': np.random.randint(20, 50, n),\n",
        "    'Soil pH': np.random.uniform(6.0, 7.5, n).round(1),\n",
        "    'Sunlight Hours': np.random.uniform(5.0, 9.0, n).round(1),\n",
        "    'Humidity': np.random.randint(50, 80, n),\n",
        "    'Plant Height': np.random.randint(20, 50, n)\n",
        "})\n",
        "\n",
        "# Separate the target variable from the input features\n",
        "inputs_df = df[['Growing Days', 'Soil pH', 'Sunlight Hours', 'Humidity']]\n",
        "targets_df = df[['Plant Height']]\n",
        "\n",
        "# Normalize the input features\n",
        "input_scaler = StandardScaler()\n",
        "inputs = input_scaler.fit_transform(inputs_df)\n",
        "\n",
        "# Normalize the target variable\n",
        "target_scaler = StandardScaler()\n",
        "targets = target_scaler.fit_transform(targets_df)\n",
        "\n",
        "# Create input and target tensors\n",
        "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
        "targets = torch.tensor(targets, dtype=torch.float32)\n",
        "\n",
        "# Split the data into train and test datasets\n",
        "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a model\n",
        "model = nn.Linear(4, 1) # Our model will have 4 input features and 1 output feature\n",
        "\n",
        "# Define the loss function (criterion) and the optimizer\n",
        "criterion = nn.MSELoss() # We use Mean Squared Error as our loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01) # We use Stochastic Gradient Descent as our optimizer\n",
        "\n",
        "# Training loop\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train() # Put the model in training mode\n",
        "    optimizer.zero_grad() # Reset the gradients\n",
        "    outputs = model(inputs_train) # Get the model's predictions\n",
        "    loss = criterion(outputs, targets_train) # Calculate the loss\n",
        "    loss.backward() # Compute gradients\n",
        "    optimizer.step() # Update model parameters\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, loss.item()))\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval() # Switch the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    predictions = model(inputs_test) # Make predictions on the test set\n",
        "    test_loss = criterion(predictions, targets_test) # Calculate the test loss\n",
        "\n",
        "    # Inverse transform the predictions\n",
        "    predictions = target_scaler.inverse_transform(predictions)\n",
        "    predictions = predictions.flatten()\n",
        "\n",
        "    for i, prediction in enumerate(predictions):\n",
        "        print(f\"Sample {i+1}: {prediction}\")\n",
        "\n",
        "    print(f'Test Loss: {test_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSdP4kEmvtdi",
        "outputId": "6d0afd0c-4e81-4456-9d71-5a3e9e192d27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.2304\n",
            "Epoch [2/100], Loss: 1.2214\n",
            "Epoch [3/100], Loss: 1.2127\n",
            "Epoch [4/100], Loss: 1.2044\n",
            "Epoch [5/100], Loss: 1.1964\n",
            "Epoch [6/100], Loss: 1.1887\n",
            "Epoch [7/100], Loss: 1.1813\n",
            "Epoch [8/100], Loss: 1.1742\n",
            "Epoch [9/100], Loss: 1.1673\n",
            "Epoch [10/100], Loss: 1.1607\n",
            "Epoch [11/100], Loss: 1.1544\n",
            "Epoch [12/100], Loss: 1.1484\n",
            "Epoch [13/100], Loss: 1.1425\n",
            "Epoch [14/100], Loss: 1.1369\n",
            "Epoch [15/100], Loss: 1.1315\n",
            "Epoch [16/100], Loss: 1.1263\n",
            "Epoch [17/100], Loss: 1.1214\n",
            "Epoch [18/100], Loss: 1.1166\n",
            "Epoch [19/100], Loss: 1.1120\n",
            "Epoch [20/100], Loss: 1.1076\n",
            "Epoch [21/100], Loss: 1.1033\n",
            "Epoch [22/100], Loss: 1.0992\n",
            "Epoch [23/100], Loss: 1.0953\n",
            "Epoch [24/100], Loss: 1.0915\n",
            "Epoch [25/100], Loss: 1.0879\n",
            "Epoch [26/100], Loss: 1.0844\n",
            "Epoch [27/100], Loss: 1.0810\n",
            "Epoch [28/100], Loss: 1.0778\n",
            "Epoch [29/100], Loss: 1.0747\n",
            "Epoch [30/100], Loss: 1.0717\n",
            "Epoch [31/100], Loss: 1.0689\n",
            "Epoch [32/100], Loss: 1.0661\n",
            "Epoch [33/100], Loss: 1.0635\n",
            "Epoch [34/100], Loss: 1.0609\n",
            "Epoch [35/100], Loss: 1.0585\n",
            "Epoch [36/100], Loss: 1.0561\n",
            "Epoch [37/100], Loss: 1.0539\n",
            "Epoch [38/100], Loss: 1.0517\n",
            "Epoch [39/100], Loss: 1.0496\n",
            "Epoch [40/100], Loss: 1.0476\n",
            "Epoch [41/100], Loss: 1.0457\n",
            "Epoch [42/100], Loss: 1.0438\n",
            "Epoch [43/100], Loss: 1.0420\n",
            "Epoch [44/100], Loss: 1.0403\n",
            "Epoch [45/100], Loss: 1.0387\n",
            "Epoch [46/100], Loss: 1.0371\n",
            "Epoch [47/100], Loss: 1.0356\n",
            "Epoch [48/100], Loss: 1.0341\n",
            "Epoch [49/100], Loss: 1.0327\n",
            "Epoch [50/100], Loss: 1.0314\n",
            "Epoch [51/100], Loss: 1.0301\n",
            "Epoch [52/100], Loss: 1.0288\n",
            "Epoch [53/100], Loss: 1.0276\n",
            "Epoch [54/100], Loss: 1.0265\n",
            "Epoch [55/100], Loss: 1.0253\n",
            "Epoch [56/100], Loss: 1.0243\n",
            "Epoch [57/100], Loss: 1.0233\n",
            "Epoch [58/100], Loss: 1.0223\n",
            "Epoch [59/100], Loss: 1.0213\n",
            "Epoch [60/100], Loss: 1.0204\n",
            "Epoch [61/100], Loss: 1.0195\n",
            "Epoch [62/100], Loss: 1.0187\n",
            "Epoch [63/100], Loss: 1.0179\n",
            "Epoch [64/100], Loss: 1.0171\n",
            "Epoch [65/100], Loss: 1.0164\n",
            "Epoch [66/100], Loss: 1.0156\n",
            "Epoch [67/100], Loss: 1.0149\n",
            "Epoch [68/100], Loss: 1.0143\n",
            "Epoch [69/100], Loss: 1.0136\n",
            "Epoch [70/100], Loss: 1.0130\n",
            "Epoch [71/100], Loss: 1.0124\n",
            "Epoch [72/100], Loss: 1.0119\n",
            "Epoch [73/100], Loss: 1.0113\n",
            "Epoch [74/100], Loss: 1.0108\n",
            "Epoch [75/100], Loss: 1.0103\n",
            "Epoch [76/100], Loss: 1.0098\n",
            "Epoch [77/100], Loss: 1.0094\n",
            "Epoch [78/100], Loss: 1.0089\n",
            "Epoch [79/100], Loss: 1.0085\n",
            "Epoch [80/100], Loss: 1.0081\n",
            "Epoch [81/100], Loss: 1.0077\n",
            "Epoch [82/100], Loss: 1.0073\n",
            "Epoch [83/100], Loss: 1.0069\n",
            "Epoch [84/100], Loss: 1.0066\n",
            "Epoch [85/100], Loss: 1.0062\n",
            "Epoch [86/100], Loss: 1.0059\n",
            "Epoch [87/100], Loss: 1.0056\n",
            "Epoch [88/100], Loss: 1.0053\n",
            "Epoch [89/100], Loss: 1.0050\n",
            "Epoch [90/100], Loss: 1.0047\n",
            "Epoch [91/100], Loss: 1.0044\n",
            "Epoch [92/100], Loss: 1.0042\n",
            "Epoch [93/100], Loss: 1.0039\n",
            "Epoch [94/100], Loss: 1.0037\n",
            "Epoch [95/100], Loss: 1.0035\n",
            "Epoch [96/100], Loss: 1.0033\n",
            "Epoch [97/100], Loss: 1.0030\n",
            "Epoch [98/100], Loss: 1.0028\n",
            "Epoch [99/100], Loss: 1.0026\n",
            "Epoch [100/100], Loss: 1.0025\n",
            "Sample 1: 34.22404423087155\n",
            "Sample 2: 34.24028530804068\n",
            "Sample 3: 34.6003197266337\n",
            "Sample 4: 34.209424050407556\n",
            "Sample 5: 34.41892725007127\n",
            "Sample 6: 34.85917464612398\n",
            "Sample 7: 34.75440697908265\n",
            "Sample 8: 34.92480286631455\n",
            "Sample 9: 33.72022708864034\n",
            "Sample 10: 34.241011852105046\n",
            "Sample 11: 33.87491244468311\n",
            "Sample 12: 34.332903596886396\n",
            "Sample 13: 34.79701979453115\n",
            "Sample 14: 34.03947624674459\n",
            "Sample 15: 34.51946864564051\n",
            "Sample 16: 34.57019880810286\n",
            "Sample 17: 33.562545479521745\n",
            "Sample 18: 34.81326997658733\n",
            "Sample 19: 34.799804853109755\n",
            "Sample 20: 33.88425865983168\n",
            "Sample 21: 34.87662395152377\n",
            "Sample 22: 34.35811569333409\n",
            "Sample 23: 34.30665031124332\n",
            "Sample 24: 34.46622963317454\n",
            "Sample 25: 33.6879138121357\n",
            "Sample 26: 34.51617622452203\n",
            "Sample 27: 34.63326866034098\n",
            "Sample 28: 34.081205434526\n",
            "Sample 29: 33.156823612611696\n",
            "Sample 30: 33.67007672279254\n",
            "Sample 31: 34.00408668488301\n",
            "Sample 32: 34.764948299759915\n",
            "Sample 33: 34.54936865725458\n",
            "Sample 34: 33.5316120632287\n",
            "Sample 35: 35.20347202891008\n",
            "Sample 36: 34.740523160408195\n",
            "Sample 37: 33.65764780674385\n",
            "Sample 38: 34.90178052760157\n",
            "Sample 39: 33.92804960230065\n",
            "Sample 40: 35.02839600292779\n",
            "Sample 41: 34.440514378311846\n",
            "Sample 42: 34.09077282390564\n",
            "Sample 43: 33.77366221259673\n",
            "Sample 44: 34.815024757254214\n",
            "Sample 45: 34.79416802078382\n",
            "Sample 46: 34.652367699988496\n",
            "Sample 47: 35.09345051802968\n",
            "Sample 48: 34.23982682351539\n",
            "Sample 49: 33.762534550154825\n",
            "Sample 50: 35.09098532936158\n",
            "Sample 51: 34.724871859587424\n",
            "Sample 52: 34.113298282039445\n",
            "Sample 53: 34.71830156572869\n",
            "Sample 54: 33.83778537227847\n",
            "Sample 55: 33.26611401078336\n",
            "Sample 56: 34.397596577499044\n",
            "Sample 57: 34.0587462703384\n",
            "Sample 58: 33.67526106491382\n",
            "Sample 59: 33.944944319634374\n",
            "Sample 60: 33.82630997443547\n",
            "Sample 61: 34.790817422353314\n",
            "Sample 62: 34.97943069807136\n",
            "Sample 63: 34.17333030167755\n",
            "Sample 64: 34.030174745987885\n",
            "Sample 65: 34.11927993361048\n",
            "Sample 66: 33.75071802686431\n",
            "Sample 67: 34.5974668188257\n",
            "Sample 68: 34.78729609919038\n",
            "Sample 69: 34.0741182034152\n",
            "Sample 70: 33.82789480801153\n",
            "Sample 71: 34.082961349253544\n",
            "Sample 72: 33.845898701477395\n",
            "Sample 73: 34.13645761528055\n",
            "Sample 74: 35.27109282608775\n",
            "Sample 75: 34.622101759400124\n",
            "Sample 76: 35.06274086810636\n",
            "Sample 77: 33.86364856535468\n",
            "Sample 78: 34.354310166468515\n",
            "Sample 79: 34.341458545508466\n",
            "Sample 80: 33.31874142433362\n",
            "Sample 81: 34.71472499761059\n",
            "Sample 82: 35.201289966587005\n",
            "Sample 83: 33.95224695746772\n",
            "Sample 84: 34.90574150988278\n",
            "Sample 85: 34.727804605266115\n",
            "Sample 86: 34.36410335542664\n",
            "Sample 87: 34.350015470636265\n",
            "Sample 88: 34.36258491300199\n",
            "Sample 89: 34.28647692732687\n",
            "Sample 90: 34.36002331297665\n",
            "Sample 91: 34.76428979933535\n",
            "Sample 92: 33.47913317922359\n",
            "Sample 93: 34.7012520653153\n",
            "Sample 94: 34.42836336382871\n",
            "Sample 95: 33.93449262215125\n",
            "Sample 96: 34.69328775168753\n",
            "Sample 97: 34.3113227626825\n",
            "Sample 98: 35.152809067642465\n",
            "Sample 99: 33.47971576238718\n",
            "Sample 100: 34.097873696146124\n",
            "Sample 101: 34.82964409527314\n",
            "Sample 102: 35.04748503043974\n",
            "Sample 103: 33.852327075744554\n",
            "Sample 104: 33.65596330543636\n",
            "Sample 105: 35.153540893189344\n",
            "Sample 106: 34.20927636330747\n",
            "Sample 107: 34.0303169895968\n",
            "Sample 108: 34.20432090702976\n",
            "Sample 109: 34.61529648817195\n",
            "Sample 110: 34.45951273767377\n",
            "Sample 111: 34.80645585968599\n",
            "Sample 112: 34.003361436887985\n",
            "Sample 113: 34.110077938576545\n",
            "Sample 114: 35.454367981058574\n",
            "Sample 115: 34.408928075194524\n",
            "Sample 116: 34.344392409046954\n",
            "Sample 117: 35.19207271033672\n",
            "Sample 118: 34.61579557206899\n",
            "Sample 119: 33.19557038285736\n",
            "Sample 120: 34.541083890485105\n",
            "Sample 121: 33.55332822327146\n",
            "Sample 122: 33.8369729312193\n",
            "Sample 123: 33.64040166021146\n",
            "Sample 124: 34.92100246702325\n",
            "Sample 125: 34.22975027329967\n",
            "Sample 126: 34.27639302191771\n",
            "Sample 127: 33.867966161111895\n",
            "Sample 128: 34.20270091757314\n",
            "Sample 129: 34.74702139281222\n",
            "Sample 130: 35.34528663886976\n",
            "Sample 131: 34.34796588279953\n",
            "Sample 132: 34.49951421643642\n",
            "Sample 133: 33.64543092763748\n",
            "Sample 134: 34.75734199288267\n",
            "Sample 135: 33.932932089875116\n",
            "Sample 136: 34.570574263186735\n",
            "Sample 137: 34.3020335178814\n",
            "Sample 138: 33.50968004283912\n",
            "Sample 139: 33.82564949750518\n",
            "Sample 140: 35.098224978225524\n",
            "Sample 141: 33.88806968689148\n",
            "Sample 142: 34.39182607992308\n",
            "Sample 143: 34.97525884531024\n",
            "Sample 144: 33.44265705763966\n",
            "Sample 145: 35.525275415645346\n",
            "Sample 146: 34.71215479492508\n",
            "Sample 147: 35.10500536492257\n",
            "Sample 148: 34.87582038853951\n",
            "Sample 149: 34.532110522080885\n",
            "Sample 150: 34.35868915540978\n",
            "Sample 151: 34.07323140037826\n",
            "Sample 152: 33.4650246221274\n",
            "Sample 153: 33.66475091470394\n",
            "Sample 154: 34.3298952903658\n",
            "Sample 155: 34.46792748399613\n",
            "Sample 156: 33.61517982704292\n",
            "Sample 157: 34.86858670159449\n",
            "Sample 158: 34.60432166470572\n",
            "Sample 159: 34.94974850710898\n",
            "Sample 160: 35.081624274219195\n",
            "Sample 161: 34.61272149002993\n",
            "Sample 162: 33.89085182931409\n",
            "Sample 163: 34.20197456791918\n",
            "Sample 164: 34.25350427555087\n",
            "Sample 165: 33.802168479856505\n",
            "Sample 166: 34.87034436601563\n",
            "Sample 167: 33.97864179818288\n",
            "Sample 168: 34.19809031294227\n",
            "Sample 169: 33.926516870711644\n",
            "Sample 170: 33.88440660614564\n",
            "Sample 171: 35.050609918396525\n",
            "Sample 172: 34.08478336351689\n",
            "Sample 173: 34.04157659189644\n",
            "Sample 174: 35.211733045209094\n",
            "Sample 175: 34.43773224398967\n",
            "Sample 176: 34.03427703222775\n",
            "Sample 177: 33.70903248421757\n",
            "Sample 178: 34.71060712613704\n",
            "Sample 179: 34.591746778848815\n",
            "Sample 180: 34.486780788898784\n",
            "Sample 181: 34.71903702026969\n",
            "Sample 182: 34.75748307002918\n",
            "Sample 183: 34.49702503428487\n",
            "Sample 184: 34.51082415482864\n",
            "Sample 185: 34.36453692301908\n",
            "Sample 186: 34.910706945894326\n",
            "Sample 187: 34.926426322756626\n",
            "Sample 188: 34.039031565357654\n",
            "Sample 189: 34.41271870536325\n",
            "Sample 190: 33.898453275931345\n",
            "Sample 191: 34.24664981849164\n",
            "Sample 192: 34.53167293667353\n",
            "Sample 193: 34.05706445837477\n",
            "Sample 194: 34.60563176398567\n",
            "Sample 195: 34.605927429801454\n",
            "Sample 196: 33.856621795878105\n",
            "Sample 197: 35.15727286002004\n",
            "Sample 198: 34.756983176088816\n",
            "Sample 199: 34.235597749293376\n",
            "Sample 200: 34.70587128880721\n",
            "Sample 201: 34.55278935683494\n",
            "Sample 202: 33.630585879144824\n",
            "Sample 203: 34.716689579500176\n",
            "Sample 204: 34.19042192434415\n",
            "Sample 205: 33.88602636879013\n",
            "Sample 206: 35.039133094877265\n",
            "Sample 207: 35.21575209139627\n",
            "Sample 208: 34.694546688630695\n",
            "Sample 209: 34.35788964264217\n",
            "Sample 210: 33.40727378171433\n",
            "Sample 211: 34.45863708083306\n",
            "Sample 212: 34.59394922186209\n",
            "Sample 213: 34.33159064625393\n",
            "Sample 214: 34.05393876037465\n",
            "Sample 215: 33.81496398911502\n",
            "Sample 216: 34.722905852021576\n",
            "Sample 217: 34.589123955748526\n",
            "Sample 218: 34.82289410140187\n",
            "Sample 219: 34.25972348042912\n",
            "Sample 220: 33.459615671191315\n",
            "Sample 221: 33.784552305530546\n",
            "Sample 222: 34.59972476600801\n",
            "Sample 223: 33.29988218991379\n",
            "Sample 224: 34.16600482062227\n",
            "Sample 225: 34.39745640355085\n",
            "Sample 226: 34.136987124405096\n",
            "Sample 227: 35.121971625283265\n",
            "Sample 228: 33.49500186327264\n",
            "Sample 229: 35.02876376260002\n",
            "Sample 230: 35.39543817117997\n",
            "Sample 231: 34.55483604471655\n",
            "Sample 232: 34.31928906901687\n",
            "Sample 233: 34.10122714592916\n",
            "Sample 234: 34.34636013390467\n",
            "Sample 235: 33.26172202064664\n",
            "Sample 236: 34.4825414109256\n",
            "Sample 237: 34.17178042957166\n",
            "Sample 238: 34.7526645110744\n",
            "Sample 239: 34.33977695225653\n",
            "Sample 240: 34.72999586968143\n",
            "Sample 241: 35.12225352036243\n",
            "Sample 242: 34.38990932704196\n",
            "Sample 243: 34.27659524113484\n",
            "Sample 244: 33.77834374462166\n",
            "Sample 245: 34.3393400067834\n",
            "Sample 246: 34.441320833150805\n",
            "Sample 247: 34.55703330345251\n",
            "Sample 248: 34.52758578204309\n",
            "Sample 249: 33.86225859580231\n",
            "Sample 250: 34.666915786596036\n",
            "Sample 251: 35.33198942117381\n",
            "Sample 252: 34.263679327035625\n",
            "Sample 253: 34.173310309808144\n",
            "Sample 254: 34.27689579961234\n",
            "Sample 255: 34.1457647215371\n",
            "Sample 256: 34.396359693985566\n",
            "Sample 257: 34.76501640820318\n",
            "Sample 258: 35.11267861378067\n",
            "Sample 259: 33.65604521701798\n",
            "Sample 260: 34.65550136881499\n",
            "Sample 261: 35.05734416502544\n",
            "Sample 262: 34.74468393177655\n",
            "Sample 263: 33.950866092802386\n",
            "Sample 264: 34.39057003078439\n",
            "Sample 265: 35.00655593865714\n",
            "Sample 266: 33.507563561624124\n",
            "Sample 267: 33.93544011363454\n",
            "Sample 268: 33.92959127676804\n",
            "Sample 269: 35.103032747403134\n",
            "Sample 270: 34.93257098744757\n",
            "Sample 271: 33.42766296117412\n",
            "Sample 272: 34.33025417196313\n",
            "Sample 273: 34.03975412400882\n",
            "Sample 274: 34.90171468727963\n",
            "Sample 275: 33.31809092713717\n",
            "Sample 276: 35.18067092923164\n",
            "Sample 277: 33.68850358848407\n",
            "Sample 278: 34.040918059070144\n",
            "Sample 279: 33.86446210807278\n",
            "Sample 280: 33.34301978383378\n",
            "Sample 281: 33.941067808671704\n",
            "Sample 282: 34.319747739852126\n",
            "Sample 283: 35.050241899510425\n",
            "Sample 284: 33.85619480583752\n",
            "Sample 285: 34.89965801966419\n",
            "Sample 286: 33.82440008667159\n",
            "Sample 287: 33.85876121752024\n",
            "Sample 288: 34.33413413371038\n",
            "Sample 289: 33.75985045537699\n",
            "Sample 290: 34.57536803481559\n",
            "Sample 291: 34.11835246639832\n",
            "Sample 292: 33.63535437742176\n",
            "Sample 293: 34.136605334782374\n",
            "Sample 294: 34.06037377699713\n",
            "Sample 295: 33.97637017746914\n",
            "Sample 296: 34.84922060686163\n",
            "Sample 297: 34.46783053801028\n",
            "Sample 298: 34.69753749821535\n",
            "Sample 299: 34.14830569785923\n",
            "Sample 300: 33.96342701351227\n",
            "Sample 301: 34.3761962979034\n",
            "Sample 302: 34.47492648518732\n",
            "Sample 303: 35.04754322395263\n",
            "Sample 304: 34.71376195329533\n",
            "Sample 305: 33.83617474692275\n",
            "Sample 306: 33.17539213868094\n",
            "Sample 307: 35.49172380969987\n",
            "Sample 308: 34.70133676344598\n",
            "Sample 309: 34.73466920406784\n",
            "Sample 310: 34.36630574983732\n",
            "Sample 311: 35.000990098528725\n",
            "Sample 312: 33.589585633156034\n",
            "Sample 313: 33.919854717939174\n",
            "Sample 314: 34.39232038051425\n",
            "Sample 315: 34.17850981594059\n",
            "Sample 316: 34.31958492924305\n",
            "Sample 317: 34.05232499205081\n",
            "Sample 318: 34.16695231210557\n",
            "Sample 319: 33.20485360903652\n",
            "Sample 320: 35.00587627990077\n",
            "Sample 321: 34.36016278623737\n",
            "Sample 322: 33.6462471272977\n",
            "Sample 323: 33.67928017590446\n",
            "Sample 324: 33.61538700372524\n",
            "Sample 325: 34.89980680842321\n",
            "Sample 326: 34.89097740091976\n",
            "Sample 327: 34.09874284023846\n",
            "Sample 328: 34.51498717811745\n",
            "Sample 329: 33.52753100012411\n",
            "Sample 330: 34.18207306694633\n",
            "Sample 331: 34.69453781055579\n",
            "Sample 332: 33.992544604273014\n",
            "Sample 333: 34.2053431169099\n",
            "Sample 334: 34.701347942043945\n",
            "Sample 335: 34.3755140794101\n",
            "Sample 336: 33.80808432350734\n",
            "Sample 337: 34.33802725056133\n",
            "Sample 338: 34.65586021801058\n",
            "Sample 339: 34.18274977714497\n",
            "Sample 340: 33.48761511053981\n",
            "Sample 341: 35.16941417828452\n",
            "Sample 342: 35.102305004474644\n",
            "Sample 343: 35.31890197229884\n",
            "Sample 344: 33.75808669943\n",
            "Sample 345: 34.407833965868335\n",
            "Sample 346: 34.92859872936319\n",
            "Sample 347: 34.28816764976715\n",
            "Sample 348: 33.27408431873178\n",
            "Sample 349: 34.4270492629347\n",
            "Sample 350: 33.30536728492298\n",
            "Sample 351: 34.83182453750955\n",
            "Sample 352: 35.03344244327242\n",
            "Sample 353: 34.73979655154037\n",
            "Sample 354: 34.6497938035054\n",
            "Sample 355: 35.311602250621476\n",
            "Sample 356: 33.71480919482589\n",
            "Sample 357: 34.843455236859946\n",
            "Sample 358: 33.49090667301236\n",
            "Sample 359: 33.992884206839065\n",
            "Sample 360: 33.98229891700981\n",
            "Sample 361: 33.307478711467596\n",
            "Sample 362: 34.14269773547761\n",
            "Sample 363: 34.81669289568642\n",
            "Sample 364: 34.78539042365182\n",
            "Sample 365: 33.75333621651675\n",
            "Sample 366: 34.3126380300389\n",
            "Sample 367: 34.47873352683393\n",
            "Sample 368: 34.54415654684791\n",
            "Sample 369: 35.029638026166204\n",
            "Sample 370: 34.72699106254802\n",
            "Sample 371: 34.71325399132339\n",
            "Sample 372: 33.88018375305641\n",
            "Sample 373: 34.54475808502544\n",
            "Sample 374: 34.765228412743745\n",
            "Sample 375: 35.125922822241044\n",
            "Sample 376: 33.483806934832536\n",
            "Sample 377: 34.275969401657385\n",
            "Sample 378: 34.536776290662594\n",
            "Sample 379: 33.93872260362179\n",
            "Sample 380: 33.5726320094713\n",
            "Sample 381: 34.23202221803074\n",
            "Sample 382: 35.235341984900586\n",
            "Sample 383: 33.433148898628374\n",
            "Sample 384: 33.90291104158654\n",
            "Sample 385: 34.899807132440536\n",
            "Sample 386: 35.277676080639786\n",
            "Sample 387: 34.35944469092495\n",
            "Sample 388: 34.98016621741583\n",
            "Sample 389: 33.644704351171384\n",
            "Sample 390: 34.4999426159524\n",
            "Sample 391: 34.01926796616146\n",
            "Sample 392: 34.13697199279567\n",
            "Sample 393: 34.62320095579844\n",
            "Sample 394: 33.434100083909264\n",
            "Sample 395: 34.216963350500464\n",
            "Sample 396: 35.137832338505945\n",
            "Sample 397: 34.97057238822187\n",
            "Sample 398: 34.88015016734842\n",
            "Sample 399: 33.87512512966008\n",
            "Sample 400: 34.22449032173388\n",
            "Sample 401: 34.387513704895134\n",
            "Sample 402: 33.64067843581676\n",
            "Sample 403: 35.38324092752393\n",
            "Sample 404: 34.0161543216061\n",
            "Sample 405: 34.237505628149805\n",
            "Sample 406: 33.37064543678766\n",
            "Sample 407: 34.92225446999551\n",
            "Sample 408: 34.61062496828259\n",
            "Sample 409: 35.297634511457936\n",
            "Sample 410: 34.620260790122835\n",
            "Sample 411: 34.81875111858506\n",
            "Sample 412: 33.83521947902348\n",
            "Sample 413: 34.68687343977259\n",
            "Sample 414: 34.56616861571945\n",
            "Sample 415: 34.81347200139406\n",
            "Sample 416: 34.81341792290129\n",
            "Sample 417: 33.70297491538557\n",
            "Sample 418: 34.0657650689886\n",
            "Sample 419: 33.422694544203125\n",
            "Sample 420: 35.13431979621272\n",
            "Sample 421: 34.17360357789567\n",
            "Sample 422: 34.67281274003639\n",
            "Sample 423: 34.82642951931876\n",
            "Sample 424: 34.20532024128624\n",
            "Sample 425: 33.77388332202433\n",
            "Sample 426: 33.939100310626124\n",
            "Sample 427: 34.95872586092638\n",
            "Sample 428: 34.97152143498836\n",
            "Sample 429: 34.54350118939147\n",
            "Sample 430: 34.214412362042765\n",
            "Sample 431: 34.19730700104128\n",
            "Sample 432: 33.98697354746555\n",
            "Sample 433: 33.90320780906125\n",
            "Sample 434: 34.47363012424226\n",
            "Sample 435: 34.0296804899491\n",
            "Sample 436: 34.54401261834887\n",
            "Sample 437: 33.948155785022365\n",
            "Sample 438: 34.51266744082986\n",
            "Sample 439: 34.354592069648106\n",
            "Sample 440: 33.7692816278701\n",
            "Sample 441: 33.561590146819\n",
            "Sample 442: 34.030695344635795\n",
            "Sample 443: 34.1933558040835\n",
            "Sample 444: 34.86062669739724\n",
            "Sample 445: 34.707983817010756\n",
            "Sample 446: 34.548857293100646\n",
            "Sample 447: 34.238316902746575\n",
            "Sample 448: 35.017445966787484\n",
            "Sample 449: 34.68904377266823\n",
            "Sample 450: 34.465957750230956\n",
            "Sample 451: 33.83435014052075\n",
            "Sample 452: 34.84074518829379\n",
            "Sample 453: 34.486926207877545\n",
            "Sample 454: 34.53575069480133\n",
            "Sample 455: 34.04240014675011\n",
            "Sample 456: 33.76321414410773\n",
            "Sample 457: 34.06827309274803\n",
            "Sample 458: 35.149824543994065\n",
            "Sample 459: 34.86706796755423\n",
            "Sample 460: 35.0913341664216\n",
            "Sample 461: 34.052256916009284\n",
            "Sample 462: 34.52721175263547\n",
            "Sample 463: 33.48256892940904\n",
            "Sample 464: 33.74165591011276\n",
            "Sample 465: 33.98982933902781\n",
            "Sample 466: 34.22303225993911\n",
            "Sample 467: 34.17793607845017\n",
            "Sample 468: 34.288457029646686\n",
            "Sample 469: 33.905305205655374\n",
            "Sample 470: 34.803894859092715\n",
            "Sample 471: 34.741330773609114\n",
            "Sample 472: 34.673938829873286\n",
            "Sample 473: 34.498906667737366\n",
            "Sample 474: 34.0714018689128\n",
            "Sample 475: 34.02960214255813\n",
            "Sample 476: 33.93301908852885\n",
            "Sample 477: 34.43290963481824\n",
            "Sample 478: 34.30709100721706\n",
            "Sample 479: 33.86972434396077\n",
            "Sample 480: 33.3596906699943\n",
            "Sample 481: 33.441926528162114\n",
            "Sample 482: 33.87154895036278\n",
            "Sample 483: 34.49510450255161\n",
            "Sample 484: 35.05017631840235\n",
            "Sample 485: 34.70273386137966\n",
            "Sample 486: 35.21525070697617\n",
            "Sample 487: 34.54074503315892\n",
            "Sample 488: 33.71226313143164\n",
            "Sample 489: 34.5486283424536\n",
            "Sample 490: 33.3762782837004\n",
            "Sample 491: 34.971607558795306\n",
            "Sample 492: 35.4314090201358\n",
            "Sample 493: 34.05972042844815\n",
            "Sample 494: 34.28582020899764\n",
            "Sample 495: 33.79312010143985\n",
            "Sample 496: 35.18958218351324\n",
            "Sample 497: 34.75448885826254\n",
            "Sample 498: 34.53562160629611\n",
            "Sample 499: 34.41513911483601\n",
            "Sample 500: 34.12323492157105\n",
            "Sample 501: 34.46558113678511\n",
            "Sample 502: 34.123531689045755\n",
            "Sample 503: 33.6577207754471\n",
            "Sample 504: 34.706821696446504\n",
            "Sample 505: 34.59751266727823\n",
            "Sample 506: 34.91546034496357\n",
            "Sample 507: 33.83171704607103\n",
            "Sample 508: 34.64936059233203\n",
            "Sample 509: 34.07411418560029\n",
            "Sample 510: 33.78901493144573\n",
            "Sample 511: 34.25080391510295\n",
            "Sample 512: 34.8163329448319\n",
            "Sample 513: 33.94567575636045\n",
            "Sample 514: 34.419896539820606\n",
            "Sample 515: 34.52194185373758\n",
            "Sample 516: 34.275667773922684\n",
            "Sample 517: 34.82298022520881\n",
            "Sample 518: 34.124186463270995\n",
            "Sample 519: 33.510568433561\n",
            "Sample 520: 35.14630688222699\n",
            "Sample 521: 34.79732680095357\n",
            "Sample 522: 34.529976171310004\n",
            "Sample 523: 34.26755327826137\n",
            "Sample 524: 33.71898623186433\n",
            "Sample 525: 34.16154592090641\n",
            "Sample 526: 35.3366729621062\n",
            "Sample 527: 34.05649577555474\n",
            "Sample 528: 34.98379151774071\n",
            "Sample 529: 35.36291979170393\n",
            "Sample 530: 34.77781107501663\n",
            "Sample 531: 34.69423300745123\n",
            "Sample 532: 34.17807398022683\n",
            "Sample 533: 34.78744628122393\n",
            "Sample 534: 34.5119410911759\n",
            "Sample 535: 34.328136491883996\n",
            "Sample 536: 34.184861203689586\n",
            "Sample 537: 35.09412492770525\n",
            "Sample 538: 34.28640509268429\n",
            "Sample 539: 34.43619855654953\n",
            "Sample 540: 34.257592045814235\n",
            "Sample 541: 33.656412069441686\n",
            "Sample 542: 34.5930119693265\n",
            "Sample 543: 34.43561218238315\n",
            "Sample 544: 34.11693764471655\n",
            "Sample 545: 34.14941464717924\n",
            "Sample 546: 34.881612003945115\n",
            "Sample 547: 34.14085307240273\n",
            "Sample 548: 34.36775869215824\n",
            "Sample 549: 33.97190353373904\n",
            "Sample 550: 34.20146403000944\n",
            "Sample 551: 34.23947532951324\n",
            "Sample 552: 34.00138911098415\n",
            "Sample 553: 34.16088855454252\n",
            "Sample 554: 34.55944037961205\n",
            "Sample 555: 34.831904440183706\n",
            "Sample 556: 34.29569410257283\n",
            "Sample 557: 35.17920883342108\n",
            "Sample 558: 34.247668302172464\n",
            "Sample 559: 34.273471373832656\n",
            "Sample 560: 33.81553374119225\n",
            "Sample 561: 34.32500147028514\n",
            "Sample 562: 34.526913510881904\n",
            "Sample 563: 34.32829929439264\n",
            "Sample 564: 34.432093418957145\n",
            "Sample 565: 34.05685378230526\n",
            "Sample 566: 35.286888411826624\n",
            "Sample 567: 34.58474314420977\n",
            "Sample 568: 34.072488169421284\n",
            "Sample 569: 34.47909984462903\n",
            "Sample 570: 33.94904294447838\n",
            "Sample 571: 33.91950092341397\n",
            "Sample 572: 35.00617042283509\n",
            "Sample 573: 33.73762681938828\n",
            "Sample 574: 34.119069063130574\n",
            "Sample 575: 34.00824616018203\n",
            "Sample 576: 34.162196418102866\n",
            "Sample 577: 34.90398378065817\n",
            "Sample 578: 33.555301812842885\n",
            "Sample 579: 34.26972908704992\n",
            "Sample 580: 34.338527565724235\n",
            "Sample 581: 33.70034784765823\n",
            "Sample 582: 35.16896923768371\n",
            "Sample 583: 34.80851058319744\n",
            "Sample 584: 33.79537461403843\n",
            "Sample 585: 35.0308091544123\n",
            "Sample 586: 34.89301780286512\n",
            "Sample 587: 34.548772530166495\n",
            "Sample 588: 34.921444232254196\n",
            "Sample 589: 34.996959906145314\n",
            "Sample 590: 33.6253256521643\n",
            "Sample 591: 33.84939607975946\n",
            "Sample 592: 34.582244225337384\n",
            "Sample 593: 34.454542182188376\n",
            "Sample 594: 34.60841769741107\n",
            "Sample 595: 34.03354154528501\n",
            "Sample 596: 33.89977228568793\n",
            "Sample 597: 34.62369971567814\n",
            "Sample 598: 34.93235214614129\n",
            "Sample 599: 34.09589770884644\n",
            "Sample 600: 34.049853614651674\n",
            "Sample 601: 34.017324542603674\n",
            "Sample 602: 33.04117405027507\n",
            "Sample 603: 34.05292030909556\n",
            "Sample 604: 33.88820071950069\n",
            "Sample 605: 34.0468487427148\n",
            "Sample 606: 33.840408811011685\n",
            "Sample 607: 34.453518546631976\n",
            "Sample 608: 34.44416082886811\n",
            "Sample 609: 34.73254776538766\n",
            "Sample 610: 34.45285633620895\n",
            "Sample 611: 34.08267235819481\n",
            "Sample 612: 34.71180709192572\n",
            "Sample 613: 34.418061621567\n",
            "Sample 614: 35.33674508836438\n",
            "Sample 615: 34.00188676920493\n",
            "Sample 616: 33.85699467502419\n",
            "Sample 617: 34.870340639816305\n",
            "Sample 618: 34.38905278722386\n",
            "Sample 619: 33.80727777956361\n",
            "Sample 620: 34.99068861548088\n",
            "Sample 621: 34.958879898766185\n",
            "Sample 622: 33.58659767492391\n",
            "Sample 623: 34.18186300650976\n",
            "Sample 624: 35.21209166759255\n",
            "Sample 625: 34.67962173746388\n",
            "Sample 626: 35.17766612209822\n",
            "Sample 627: 33.6096290861222\n",
            "Sample 628: 34.001235170349545\n",
            "Sample 629: 34.818828137504966\n",
            "Sample 630: 33.90014114701909\n",
            "Sample 631: 33.230507875736826\n",
            "Sample 632: 34.74988233625006\n",
            "Sample 633: 34.33903261154019\n",
            "Sample 634: 34.71478014536056\n",
            "Sample 635: 34.189332999295274\n",
            "Sample 636: 34.29693638502511\n",
            "Sample 637: 33.99311403233291\n",
            "Sample 638: 34.324132285690645\n",
            "Sample 639: 34.315947121849135\n",
            "Sample 640: 35.2468904162503\n",
            "Sample 641: 33.90458886813698\n",
            "Sample 642: 33.603188723213734\n",
            "Sample 643: 34.89353357365478\n",
            "Sample 644: 34.439416324074635\n",
            "Sample 645: 34.60988348701922\n",
            "Sample 646: 35.170364326709944\n",
            "Sample 647: 33.932445545448836\n",
            "Sample 648: 34.042463103317786\n",
            "Sample 649: 34.735487833858066\n",
            "Sample 650: 34.6742796637052\n",
            "Sample 651: 33.590393213955224\n",
            "Sample 652: 34.30453865788657\n",
            "Sample 653: 33.74800016948045\n",
            "Sample 654: 35.219115780127005\n",
            "Sample 655: 35.05464011077993\n",
            "Sample 656: 34.63386987450118\n",
            "Sample 657: 34.44072417143412\n",
            "Sample 658: 34.00650735356964\n",
            "Sample 659: 35.10230617093704\n",
            "Sample 660: 33.61590355215649\n",
            "Sample 661: 33.70905361014764\n",
            "Sample 662: 33.82133455869009\n",
            "Sample 663: 34.08946871894634\n",
            "Sample 664: 34.180190299433164\n",
            "Sample 665: 33.87593355290433\n",
            "Sample 666: 34.89863691144297\n",
            "Sample 667: 34.615514778648766\n",
            "Sample 668: 34.31295250506083\n",
            "Sample 669: 34.53589953216295\n",
            "Sample 670: 33.86027113828906\n",
            "Sample 671: 33.7818384659616\n",
            "Sample 672: 34.214401215846536\n",
            "Sample 673: 35.09463577343146\n",
            "Sample 674: 34.19202901791094\n",
            "Sample 675: 34.495050197246705\n",
            "Sample 676: 34.27690953794723\n",
            "Sample 677: 34.43209680493827\n",
            "Sample 678: 34.42646948252104\n",
            "Sample 679: 34.348677360057934\n",
            "Sample 680: 34.30964604589142\n",
            "Sample 681: 33.08796189385454\n",
            "Sample 682: 33.52190320788175\n",
            "Sample 683: 34.969267116800175\n",
            "Sample 684: 35.165758809151185\n",
            "Sample 685: 34.55739330290964\n",
            "Sample 686: 34.47933080418354\n",
            "Sample 687: 34.438549804522694\n",
            "Sample 688: 34.85600873757292\n",
            "Sample 689: 35.013489974373186\n",
            "Sample 690: 33.3835091192929\n",
            "Sample 691: 33.90998881139123\n",
            "Sample 692: 33.89714398669473\n",
            "Sample 693: 34.15124067925751\n",
            "Sample 694: 33.784864269418144\n",
            "Sample 695: 35.02459677009889\n",
            "Sample 696: 34.82092951951228\n",
            "Sample 697: 34.66287295753666\n",
            "Sample 698: 34.44649717204398\n",
            "Sample 699: 34.83811585244512\n",
            "Sample 700: 33.86036186314212\n",
            "Sample 701: 34.47588637033358\n",
            "Sample 702: 34.17572478976372\n",
            "Sample 703: 33.89412835738242\n",
            "Sample 704: 35.215322509217025\n",
            "Sample 705: 33.86533526998004\n",
            "Sample 706: 34.90059922521124\n",
            "Sample 707: 34.00064338509372\n",
            "Sample 708: 34.50869647881481\n",
            "Sample 709: 34.24847047188219\n",
            "Sample 710: 35.125561089291196\n",
            "Sample 711: 34.455846044134674\n",
            "Sample 712: 34.353719733985045\n",
            "Sample 713: 34.349408812984876\n",
            "Sample 714: 34.55981436041706\n",
            "Sample 715: 34.508353636075434\n",
            "Sample 716: 34.388965820971855\n",
            "Sample 717: 34.6082178759222\n",
            "Sample 718: 34.14941199023712\n",
            "Sample 719: 33.49654172324296\n",
            "Sample 720: 34.819476560990495\n",
            "Sample 721: 33.87527793623402\n",
            "Sample 722: 34.42762274121126\n",
            "Sample 723: 34.71347409629726\n",
            "Sample 724: 34.132949998050776\n",
            "Sample 725: 34.274856207710314\n",
            "Sample 726: 35.070727182511\n",
            "Sample 727: 35.353266861748544\n",
            "Sample 728: 35.496913012081144\n",
            "Sample 729: 34.3251356539629\n",
            "Sample 730: 34.81858940153448\n",
            "Sample 731: 34.43591269225805\n",
            "Sample 732: 34.11863951335306\n",
            "Sample 733: 34.44064234085683\n",
            "Sample 734: 34.7270642580634\n",
            "Sample 735: 34.40606479883189\n",
            "Sample 736: 34.600151756048604\n",
            "Sample 737: 35.23593040037616\n",
            "Sample 738: 34.01548098118772\n",
            "Sample 739: 34.52750592797153\n",
            "Sample 740: 33.789975318818854\n",
            "Sample 741: 33.661904098421786\n",
            "Sample 742: 33.95650376757339\n",
            "Sample 743: 34.39371397096031\n",
            "Sample 744: 34.18143368354438\n",
            "Sample 745: 34.25357523534665\n",
            "Sample 746: 34.9806541227149\n",
            "Sample 747: 34.61506896320117\n",
            "Sample 748: 34.05429738275811\n",
            "Sample 749: 34.08501205495008\n",
            "Sample 750: 34.21140288905977\n",
            "Sample 751: 33.6475523987194\n",
            "Sample 752: 34.54642879939544\n",
            "Sample 753: 34.824798610478034\n",
            "Sample 754: 33.420429209425635\n",
            "Sample 755: 33.86722694796984\n",
            "Sample 756: 34.652945228481705\n",
            "Sample 757: 35.50509704186199\n",
            "Sample 758: 35.18958134106818\n",
            "Sample 759: 33.97695395949686\n",
            "Sample 760: 34.7302848931419\n",
            "Sample 761: 33.68258178291432\n",
            "Sample 762: 34.86808126695773\n",
            "Sample 763: 34.7453586978712\n",
            "Sample 764: 33.455739387040786\n",
            "Sample 765: 34.77451915612501\n",
            "Sample 766: 34.38992445055095\n",
            "Sample 767: 33.465396399614555\n",
            "Sample 768: 34.69651240458095\n",
            "Sample 769: 33.42569027885123\n",
            "Sample 770: 34.6739246055124\n",
            "Sample 771: 33.56041001089107\n",
            "Sample 772: 35.01413736100325\n",
            "Sample 773: 34.613984639198414\n",
            "Sample 774: 33.87080241442901\n",
            "Sample 775: 34.49379952274552\n",
            "Sample 776: 33.71942579377741\n",
            "Sample 777: 34.26388669812833\n",
            "Sample 778: 33.99319617072666\n",
            "Sample 779: 34.06893049151366\n",
            "Sample 780: 33.71950297470598\n",
            "Sample 781: 34.43173452115895\n",
            "Sample 782: 34.244591595593\n",
            "Sample 783: 34.52471401642635\n",
            "Sample 784: 34.068403250510435\n",
            "Sample 785: 34.87407186140714\n",
            "Sample 786: 34.35723227627827\n",
            "Sample 787: 34.910186638862015\n",
            "Sample 788: 34.478065014273795\n",
            "Sample 789: 35.22979506738439\n",
            "Sample 790: 34.714931915079035\n",
            "Sample 791: 35.30318745569652\n",
            "Sample 792: 33.52234795407215\n",
            "Sample 793: 34.282456779480775\n",
            "Sample 794: 34.84892413100252\n",
            "Sample 795: 34.13952920238616\n",
            "Sample 796: 34.05789488239588\n",
            "Sample 797: 34.46902327821245\n",
            "Sample 798: 34.774883157196186\n",
            "Sample 799: 34.27543623113698\n",
            "Sample 800: 33.6189081648795\n",
            "Sample 801: 34.53590492705153\n",
            "Sample 802: 34.18514974112406\n",
            "Sample 803: 34.85462100374014\n",
            "Sample 804: 34.11315211782081\n",
            "Sample 805: 34.8107730666224\n",
            "Sample 806: 33.76700112908146\n",
            "Sample 807: 33.6083820082134\n",
            "Sample 808: 35.26809598978072\n",
            "Sample 809: 34.24152354027631\n",
            "Sample 810: 34.51770122829767\n",
            "Sample 811: 34.61286943634388\n",
            "Sample 812: 34.17455346710723\n",
            "Sample 813: 34.191885121813634\n",
            "Sample 814: 34.33217439587991\n",
            "Sample 815: 35.3238064930519\n",
            "Sample 816: 34.69935555946724\n",
            "Sample 817: 34.9849106736067\n",
            "Sample 818: 33.704733487055236\n",
            "Sample 819: 34.12485691993513\n",
            "Sample 820: 33.526723678538794\n",
            "Sample 821: 34.96297871802059\n",
            "Sample 822: 34.36555916530096\n",
            "Sample 823: 34.28771295624465\n",
            "Sample 824: 33.6110199629231\n",
            "Sample 825: 33.913516388088674\n",
            "Sample 826: 33.970082880348485\n",
            "Sample 827: 34.14342321028477\n",
            "Sample 828: 34.09889298987028\n",
            "Sample 829: 33.745517613483344\n",
            "Sample 830: 34.71208010892997\n",
            "Sample 831: 34.994620695416046\n",
            "Sample 832: 34.0032678606824\n",
            "Sample 833: 34.5560791696139\n",
            "Sample 834: 33.53169799262524\n",
            "Sample 835: 34.28261554797363\n",
            "Sample 836: 34.66821506369708\n",
            "Sample 837: 34.12228188939136\n",
            "Sample 838: 34.25278168449796\n",
            "Sample 839: 34.73832366595264\n",
            "Sample 840: 34.233548404469644\n",
            "Sample 841: 34.32573829379937\n",
            "Sample 842: 34.24606227786287\n",
            "Sample 843: 34.76012390849315\n",
            "Sample 844: 33.67059839069764\n",
            "Sample 845: 33.96702020371952\n",
            "Sample 846: 34.994916588043964\n",
            "Sample 847: 34.284790222701524\n",
            "Sample 848: 33.717004736269985\n",
            "Sample 849: 33.758392377381355\n",
            "Sample 850: 33.693608675965855\n",
            "Sample 851: 34.52668235691699\n",
            "Sample 852: 33.9692086167823\n",
            "Sample 853: 33.73367341911265\n",
            "Sample 854: 34.292038765841234\n",
            "Sample 855: 35.2459551078187\n",
            "Sample 856: 34.21009366462662\n",
            "Sample 857: 34.77897695418194\n",
            "Sample 858: 33.778928401496174\n",
            "Sample 859: 33.58644493315344\n",
            "Sample 860: 34.215577431164746\n",
            "Sample 861: 33.930100307997186\n",
            "Sample 862: 34.74380811292717\n",
            "Sample 863: 34.020165202556505\n",
            "Sample 864: 33.82644794101559\n",
            "Sample 865: 34.29282232075522\n",
            "Sample 866: 34.54122671732521\n",
            "Sample 867: 33.69522934585888\n",
            "Sample 868: 34.10510825793795\n",
            "Sample 869: 34.172718783766186\n",
            "Sample 870: 33.37284382958428\n",
            "Sample 871: 34.66925619618944\n",
            "Sample 872: 33.47409211756667\n",
            "Sample 873: 33.673867077546525\n",
            "Sample 874: 34.635616101110486\n",
            "Sample 875: 33.83764371190076\n",
            "Sample 876: 34.23916832309083\n",
            "Sample 877: 35.219187647171324\n",
            "Sample 878: 34.07550188703133\n",
            "Sample 879: 34.049257195947995\n",
            "Sample 880: 34.000925539386735\n",
            "Sample 881: 34.14537291977881\n",
            "Sample 882: 33.81571175631465\n",
            "Sample 883: 33.84567869370872\n",
            "Sample 884: 34.240701378697175\n",
            "Sample 885: 34.23711904307056\n",
            "Sample 886: 33.78953180389431\n",
            "Sample 887: 34.354812968464444\n",
            "Sample 888: 34.95478380125738\n",
            "Sample 889: 33.715240980322996\n",
            "Sample 890: 34.47283255557865\n",
            "Sample 891: 34.102704049331805\n",
            "Sample 892: 34.280416053518074\n",
            "Sample 893: 34.09479076843388\n",
            "Sample 894: 34.742066228150115\n",
            "Sample 895: 33.727467903966684\n",
            "Sample 896: 33.783170371608016\n",
            "Sample 897: 34.6680733385159\n",
            "Sample 898: 33.99457010142109\n",
            "Sample 899: 35.12598691286939\n",
            "Sample 900: 34.578601274168314\n",
            "Sample 901: 34.11804283543551\n",
            "Sample 902: 33.898660128596326\n",
            "Sample 903: 34.48545150779276\n",
            "Sample 904: 33.94567575636045\n",
            "Sample 905: 34.1798995262791\n",
            "Sample 906: 34.23112643971369\n",
            "Sample 907: 33.55552635685421\n",
            "Sample 908: 33.588931118144664\n",
            "Sample 909: 34.23684997907777\n",
            "Sample 910: 34.36009431327461\n",
            "Sample 911: 33.735490249098675\n",
            "Sample 912: 34.493492532523966\n",
            "Sample 913: 34.77707668973282\n",
            "Sample 914: 34.071703755861364\n",
            "Sample 915: 34.50930890397471\n",
            "Sample 916: 34.44262067728218\n",
            "Sample 917: 34.710405133732046\n",
            "Sample 918: 33.633677393316376\n",
            "Sample 919: 35.28951230418411\n",
            "Sample 920: 33.8614701968292\n",
            "Sample 921: 35.15618286571397\n",
            "Sample 922: 34.742572505231934\n",
            "Sample 923: 34.189102946989294\n",
            "Sample 924: 34.655280453797786\n",
            "Sample 925: 34.35336604476547\n",
            "Sample 926: 34.103785197964704\n",
            "Sample 927: 34.03807004392387\n",
            "Sample 928: 33.46605091462593\n",
            "Sample 929: 34.30007266219113\n",
            "Sample 930: 33.75238179106254\n",
            "Sample 931: 34.13106104180656\n",
            "Sample 932: 33.51669398122934\n",
            "Sample 933: 33.7001990588992\n",
            "Sample 934: 34.24567792850322\n",
            "Sample 935: 34.32384267089854\n",
            "Sample 936: 34.6711400329598\n",
            "Sample 937: 33.451491649420426\n",
            "Sample 938: 34.35409637173218\n",
            "Sample 939: 34.52253023681142\n",
            "Sample 940: 35.013842829248134\n",
            "Sample 941: 34.67260154553916\n",
            "Sample 942: 35.28082572352076\n",
            "Sample 943: 34.28985412758037\n",
            "Sample 944: 34.317897933002094\n",
            "Sample 945: 34.657115947182156\n",
            "Sample 946: 34.63021965724344\n",
            "Sample 947: 34.92933133255167\n",
            "Sample 948: 34.72319293137805\n",
            "Sample 949: 34.36470490170461\n",
            "Sample 950: 34.80075639480971\n",
            "Sample 951: 33.969796125009346\n",
            "Sample 952: 34.30679430454582\n",
            "Sample 953: 33.71504527385425\n",
            "Sample 954: 33.97528724674092\n",
            "Sample 955: 34.84118588426754\n",
            "Sample 956: 34.076884112569466\n",
            "Sample 957: 35.09864970014478\n",
            "Sample 958: 34.19114474220918\n",
            "Sample 959: 33.6058965360603\n",
            "Sample 960: 34.710923917882894\n",
            "Sample 961: 34.35914166991572\n",
            "Sample 962: 34.0330339073304\n",
            "Sample 963: 33.609185311983794\n",
            "Sample 964: 34.50636752323411\n",
            "Sample 965: 34.24245590015019\n",
            "Sample 966: 34.71508494846511\n",
            "Sample 967: 35.22073560757496\n",
            "Sample 968: 34.4243540381615\n",
            "Sample 969: 33.560127273366845\n",
            "Sample 970: 33.55538275237251\n",
            "Sample 971: 34.190068194622405\n",
            "Sample 972: 34.718214826288815\n",
            "Sample 973: 33.98616625828197\n",
            "Sample 974: 33.66034790797791\n",
            "Sample 975: 35.33418781397043\n",
            "Sample 976: 33.95100778558183\n",
            "Sample 977: 34.36754265360189\n",
            "Sample 978: 33.695889239557964\n",
            "Sample 979: 33.74573995417681\n",
            "Sample 980: 33.178678840893504\n",
            "Sample 981: 33.99998887008234\n",
            "Sample 982: 34.2592100263634\n",
            "Sample 983: 34.18209681741679\n",
            "Sample 984: 33.630896352552696\n",
            "Sample 985: 34.41916857818042\n",
            "Sample 986: 34.2043738838636\n",
            "Sample 987: 33.88712329706711\n",
            "Sample 988: 34.39899167867592\n",
            "Sample 989: 34.105409010825845\n",
            "Sample 990: 34.69914060636895\n",
            "Sample 991: 34.05890682092659\n",
            "Sample 992: 33.96000746419344\n",
            "Sample 993: 34.780578085829816\n",
            "Sample 994: 34.956024009998735\n",
            "Sample 995: 33.58038224484758\n",
            "Sample 996: 34.41066910132565\n",
            "Sample 997: 33.50845182273906\n",
            "Sample 998: 34.55483230231636\n",
            "Sample 999: 34.565131274229884\n",
            "Sample 1000: 34.18427632000294\n",
            "Sample 1001: 35.15801343403489\n",
            "Sample 1002: 34.15291740414903\n",
            "Sample 1003: 33.58981348214415\n",
            "Sample 1004: 33.70632953163099\n",
            "Sample 1005: 34.40089821286042\n",
            "Sample 1006: 34.63898724223987\n",
            "Sample 1007: 33.75298368565914\n",
            "Sample 1008: 34.065852262052736\n",
            "Sample 1009: 34.29093090197928\n",
            "Sample 1010: 34.18852457605102\n",
            "Sample 1011: 33.30376132541685\n",
            "Sample 1012: 34.11505196104739\n",
            "Sample 1013: 34.39195667105867\n",
            "Sample 1014: 33.783672857687044\n",
            "Sample 1015: 35.116706538042756\n",
            "Sample 1016: 34.84730806215562\n",
            "Sample 1017: 34.19276602773513\n",
            "Sample 1018: 33.80853898462811\n",
            "Sample 1019: 34.53808817203787\n",
            "Sample 1020: 33.95049988841335\n",
            "Sample 1021: 33.10967312883385\n",
            "Sample 1022: 34.50607161440533\n",
            "Sample 1023: 35.34230179120402\n",
            "Sample 1024: 34.87502537961282\n",
            "Sample 1025: 33.51539590299242\n",
            "Sample 1026: 35.47381083549747\n",
            "Sample 1027: 34.53714805194888\n",
            "Sample 1028: 34.11423870994489\n",
            "Sample 1029: 34.20248625609045\n",
            "Sample 1030: 34.10444399000487\n",
            "Sample 1031: 35.293831260814116\n",
            "Sample 1032: 33.70084949129219\n",
            "Sample 1033: 33.75450727995938\n",
            "Sample 1034: 34.816974531551715\n",
            "Sample 1035: 33.9434873756994\n",
            "Sample 1036: 33.903273422571054\n",
            "Sample 1037: 34.12374774380298\n",
            "Sample 1038: 34.508993230088656\n",
            "Sample 1039: 34.60929481232978\n",
            "Sample 1040: 35.0907463989807\n",
            "Sample 1041: 33.387316387751646\n",
            "Sample 1042: 34.23479156176872\n",
            "Sample 1043: 33.855967280866736\n",
            "Sample 1044: 34.295097732471746\n",
            "Sample 1045: 34.17289073976447\n",
            "Sample 1046: 34.87276373863292\n",
            "Sample 1047: 35.05398442930616\n",
            "Sample 1048: 34.60914450068929\n",
            "Sample 1049: 34.031136040609546\n",
            "Sample 1050: 33.844581765431734\n",
            "Sample 1051: 34.21577449850629\n",
            "Sample 1052: 34.39246945278844\n",
            "Sample 1053: 34.585558209809335\n",
            "Sample 1054: 33.96476484867587\n",
            "Sample 1055: 33.76627176606631\n",
            "Sample 1056: 34.45446608671787\n",
            "Sample 1057: 34.53574781104707\n",
            "Sample 1058: 34.7082808112976\n",
            "Sample 1059: 33.78631599667407\n",
            "Sample 1060: 35.04894200677645\n",
            "Sample 1061: 34.57978144249798\n",
            "Sample 1062: 34.09620186391633\n",
            "Sample 1063: 34.62222569602975\n",
            "Sample 1064: 35.32007420220386\n",
            "Sample 1065: 34.462010538686364\n",
            "Sample 1066: 33.60975305515356\n",
            "Sample 1067: 34.031867736549486\n",
            "Sample 1068: 35.55612296734532\n",
            "Sample 1069: 33.85920304755465\n",
            "Sample 1070: 35.23871656061369\n",
            "Sample 1071: 34.18157764444514\n",
            "Sample 1072: 34.74652370218971\n",
            "Sample 1073: 35.0802291203895\n",
            "Sample 1074: 34.07124497972047\n",
            "Sample 1075: 34.6758945660897\n",
            "Sample 1076: 34.595778688524085\n",
            "Sample 1077: 34.11469337106567\n",
            "Sample 1078: 34.435034402776516\n",
            "Sample 1079: 34.75374339158597\n",
            "Sample 1080: 33.92688453317867\n",
            "Sample 1081: 33.6012153280527\n",
            "Sample 1082: 33.78258772364096\n",
            "Sample 1083: 33.25214717886835\n",
            "Sample 1084: 34.80930876749398\n",
            "Sample 1085: 34.72062904703052\n",
            "Sample 1086: 33.88930393371392\n",
            "Sample 1087: 33.917973084486675\n",
            "Sample 1088: 33.69991230356006\n",
            "Sample 1089: 34.80761830426758\n",
            "Sample 1090: 33.91994025851492\n",
            "Sample 1091: 34.766985007907685\n",
            "Sample 1092: 35.08950327408335\n",
            "Sample 1093: 33.943636196860155\n",
            "Sample 1094: 34.07514015408148\n",
            "Sample 1095: 34.97604607782096\n",
            "Sample 1096: 35.406998720777906\n",
            "Sample 1097: 34.37698361141844\n",
            "Sample 1098: 34.23612456907407\n",
            "Sample 1099: 34.5805661476735\n",
            "Sample 1100: 33.69734705833974\n",
            "Sample 1101: 33.81225500980606\n",
            "Sample 1102: 34.34716345387593\n",
            "Sample 1103: 34.486069084828245\n",
            "Sample 1104: 34.053421985131266\n",
            "Sample 1105: 34.00357723243134\n",
            "Sample 1106: 34.28040315762825\n",
            "Sample 1107: 33.810290492719936\n",
            "Sample 1108: 34.58948429542385\n",
            "Sample 1109: 35.10590341136095\n",
            "Sample 1110: 34.89000729302666\n",
            "Sample 1111: 33.83367670289716\n",
            "Sample 1112: 33.92102082391662\n",
            "Sample 1113: 34.5568035103604\n",
            "Sample 1114: 34.65543037661747\n",
            "Sample 1115: 33.14673215759869\n",
            "Sample 1116: 34.13610741734773\n",
            "Sample 1117: 35.07553644216834\n",
            "Sample 1118: 34.614499178722205\n",
            "Sample 1119: 33.93639272459169\n",
            "Sample 1120: 34.70843251621262\n",
            "Sample 1121: 34.8323971409393\n",
            "Sample 1122: 34.93138715772205\n",
            "Sample 1123: 34.0368320709021\n",
            "Sample 1124: 34.55448579818113\n",
            "Sample 1125: 33.46786657814956\n",
            "Sample 1126: 34.78321493888059\n",
            "Sample 1127: 34.57479828273836\n",
            "Sample 1128: 34.042748983810135\n",
            "Sample 1129: 34.46016318626763\n",
            "Sample 1130: 34.22135527583372\n",
            "Sample 1131: 34.2254315434818\n",
            "Sample 1132: 34.09983714397505\n",
            "Sample 1133: 34.472752976921825\n",
            "Sample 1134: 33.713802084153436\n",
            "Sample 1135: 35.113694602528035\n",
            "Sample 1136: 34.4451201145823\n",
            "Sample 1137: 34.1119755136818\n",
            "Sample 1138: 34.51682958927187\n",
            "Sample 1139: 34.50169632736209\n",
            "Sample 1140: 34.122070986509726\n",
            "Sample 1141: 34.32887794884608\n",
            "Sample 1142: 34.0597824777673\n",
            "Sample 1143: 33.83551621409646\n",
            "Sample 1144: 34.0535659136303\n",
            "Sample 1145: 34.672378297597156\n",
            "Sample 1146: 34.447003967553535\n",
            "Sample 1147: 34.293552655822374\n",
            "Sample 1148: 34.147647165032936\n",
            "Sample 1149: 35.15749695040711\n",
            "Sample 1150: 34.28866129017301\n",
            "Sample 1151: 34.99075069720176\n",
            "Sample 1152: 35.23476445640738\n",
            "Sample 1153: 33.63908271525834\n",
            "Sample 1154: 34.24436377900662\n",
            "Sample 1155: 34.66815408363513\n",
            "Sample 1156: 33.5053093082394\n",
            "Sample 1157: 34.94622313372939\n",
            "Sample 1158: 34.174039154395565\n",
            "Sample 1159: 34.40118119339764\n",
            "Sample 1160: 34.1977128651518\n",
            "Sample 1161: 33.68609937987792\n",
            "Sample 1162: 34.80125777922981\n",
            "Sample 1163: 35.35553718639295\n",
            "Sample 1164: 33.887563182997525\n",
            "Sample 1165: 35.00814161467826\n",
            "Sample 1166: 34.76918336830258\n",
            "Sample 1167: 35.0863547328613\n",
            "Sample 1168: 34.10869548622628\n",
            "Sample 1169: 35.228400237572025\n",
            "Sample 1170: 34.302421237021306\n",
            "Sample 1171: 34.023382046632214\n",
            "Sample 1172: 34.48136662128365\n",
            "Sample 1173: 33.66671445973806\n",
            "Sample 1174: 34.09918155970648\n",
            "Sample 1175: 35.171985191013356\n",
            "Sample 1176: 34.1586209516437\n",
            "Sample 1177: 33.813220322242636\n",
            "Sample 1178: 33.181454762183336\n",
            "Sample 1179: 35.30655205167579\n",
            "Sample 1180: 33.766792461919415\n",
            "Sample 1181: 34.70212522722255\n",
            "Sample 1182: 33.79149444167991\n",
            "Sample 1183: 34.6793478132111\n",
            "Sample 1184: 34.1206078538437\n",
            "Sample 1185: 34.17141383636182\n",
            "Sample 1186: 35.16450058504615\n",
            "Sample 1187: 35.12130227027807\n",
            "Sample 1188: 34.315788304753674\n",
            "Sample 1189: 34.671794288757305\n",
            "Sample 1190: 34.339037731014045\n",
            "Sample 1191: 33.984934279580855\n",
            "Sample 1192: 33.97592173748115\n",
            "Sample 1193: 34.80850659778425\n",
            "Sample 1194: 33.31194652166009\n",
            "Sample 1195: 33.12575138729347\n",
            "Sample 1196: 33.82022641941341\n",
            "Sample 1197: 34.29122520692227\n",
            "Sample 1198: 35.18753685650442\n",
            "Sample 1199: 34.03947404342673\n",
            "Sample 1200: 34.71559148476079\n",
            "Sample 1201: 33.80934118673957\n",
            "Sample 1202: 34.27690347882312\n",
            "Sample 1203: 34.31915397809057\n",
            "Sample 1204: 34.685686629087606\n",
            "Sample 1205: 34.24838350563019\n",
            "Sample 1206: 34.52406037626177\n",
            "Sample 1207: 34.81639619301518\n",
            "Sample 1208: 34.12169843138098\n",
            "Sample 1209: 34.02878105023619\n",
            "Sample 1210: 34.2749941418887\n",
            "Sample 1211: 33.976753036349066\n",
            "Sample 1212: 33.39711110769167\n",
            "Sample 1213: 34.64972974527879\n",
            "Sample 1214: 34.39496974873449\n",
            "Sample 1215: 33.982601873215565\n",
            "Sample 1216: 33.97409995002994\n",
            "Sample 1217: 34.63648411114216\n",
            "Sample 1218: 34.68495486834419\n",
            "Sample 1219: 35.622792255224766\n",
            "Sample 1220: 34.430200655509275\n",
            "Sample 1221: 34.506213566398635\n",
            "Sample 1222: 34.00151819948937\n",
            "Sample 1223: 34.731822549794366\n",
            "Sample 1224: 34.47018795851363\n",
            "Sample 1225: 34.71654075833942\n",
            "Sample 1226: 34.190216173338094\n",
            "Sample 1227: 34.36754892333727\n",
            "Sample 1228: 34.22316215848765\n",
            "Sample 1229: 34.88074285985278\n",
            "Sample 1230: 34.16784462343717\n",
            "Sample 1231: 34.513013426537356\n",
            "Sample 1232: 33.7657742698542\n",
            "Sample 1233: 34.577941623482225\n",
            "Sample 1234: 34.67413200900685\n",
            "Sample 1235: 34.916479962705054\n",
            "Sample 1236: 34.31360076653769\n",
            "Sample 1237: 35.06808806133889\n",
            "Sample 1238: 34.92012301917975\n",
            "Sample 1239: 34.07220650115426\n",
            "Sample 1240: 34.18999635997982\n",
            "Sample 1241: 35.21064901282194\n",
            "Sample 1242: 34.466674338862774\n",
            "Sample 1243: 33.98696473419411\n",
            "Sample 1244: 34.1409291840741\n",
            "Sample 1245: 34.728301129426235\n",
            "Sample 1246: 34.98607311818829\n",
            "Sample 1247: 34.09912433824559\n",
            "Sample 1248: 34.65908085308908\n",
            "Sample 1249: 34.93724863126451\n",
            "Sample 1250: 33.96979722666828\n",
            "Sample 1251: 34.02090175875644\n",
            "Sample 1252: 34.55871602266468\n",
            "Sample 1253: 34.6459831652664\n",
            "Sample 1254: 33.9431965701436\n",
            "Sample 1255: 33.30354151205857\n",
            "Sample 1256: 34.94929864144472\n",
            "Sample 1257: 34.31308935378115\n",
            "Sample 1258: 34.736050522357445\n",
            "Sample 1259: 33.87073650930361\n",
            "Sample 1260: 34.761500139710634\n",
            "Sample 1261: 33.60632754391581\n",
            "Sample 1262: 34.176833091049076\n",
            "Sample 1263: 33.692668572077736\n",
            "Sample 1264: 33.622698519633495\n",
            "Sample 1265: 34.33851868764933\n",
            "Sample 1266: 34.51630792136677\n",
            "Sample 1267: 33.77300990090321\n",
            "Sample 1268: 34.07872631311262\n",
            "Sample 1269: 33.458444348534826\n",
            "Sample 1270: 35.09695703360052\n",
            "Sample 1271: 34.75470374655737\n",
            "Sample 1272: 34.17888353753174\n",
            "Sample 1273: 33.63959809722719\n",
            "Sample 1274: 33.98296049559902\n",
            "Sample 1275: 33.280730173432424\n",
            "Sample 1276: 34.099778010811896\n",
            "Sample 1277: 34.56786022920738\n",
            "Sample 1278: 34.16125252321196\n",
            "Sample 1279: 34.89812489925437\n",
            "Sample 1280: 34.98221173883504\n",
            "Sample 1281: 34.33888754088006\n",
            "Sample 1282: 34.889649513088266\n",
            "Sample 1283: 34.040413758494054\n",
            "Sample 1284: 33.759858426203365\n",
            "Sample 1285: 34.18017656109827\n",
            "Sample 1286: 34.604402345021484\n",
            "Sample 1287: 33.86320096781176\n",
            "Sample 1288: 34.744341283447575\n",
            "Sample 1289: 33.83997378534125\n",
            "Sample 1290: 33.46085652796734\n",
            "Sample 1291: 33.68724555878943\n",
            "Sample 1292: 33.85678348052696\n",
            "Sample 1293: 33.37086032508248\n",
            "Sample 1294: 34.27669085864962\n",
            "Sample 1295: 34.04962981588021\n",
            "Sample 1296: 34.026592410361275\n",
            "Sample 1297: 34.31777065899394\n",
            "Sample 1298: 34.82891466745452\n",
            "Sample 1299: 34.48940497287187\n",
            "Sample 1300: 34.58692928915123\n",
            "Sample 1301: 34.31476205275731\n",
            "Sample 1302: 34.612158866334006\n",
            "Sample 1303: 33.77738452371093\n",
            "Sample 1304: 34.37002668387785\n",
            "Sample 1305: 34.60703394899147\n",
            "Sample 1306: 34.823342800603726\n",
            "Sample 1307: 34.8693273404128\n",
            "Sample 1308: 34.10393343589426\n",
            "Sample 1309: 34.682338622795754\n",
            "Sample 1310: 34.03267129953375\n",
            "Sample 1311: 34.05862009798918\n",
            "Sample 1312: 34.76194083568438\n",
            "Sample 1313: 34.10312987291\n",
            "Sample 1314: 34.60988348701922\n",
            "Sample 1315: 33.896776291825965\n",
            "Sample 1316: 35.30143465153537\n",
            "Sample 1317: 34.138737919658794\n",
            "Sample 1318: 33.957655130762205\n",
            "Sample 1319: 34.34796479734147\n",
            "Sample 1320: 33.48864133823487\n",
            "Sample 1321: 33.698803969872976\n",
            "Sample 1322: 33.950559086379975\n",
            "Sample 1323: 34.69439501611742\n",
            "Sample 1324: 34.454385341598645\n",
            "Sample 1325: 34.05130057885281\n",
            "Sample 1326: 34.112199344855\n",
            "Sample 1327: 35.2955849722238\n",
            "Sample 1328: 34.260837533022126\n",
            "Sample 1329: 34.31893679737312\n",
            "Sample 1330: 34.38700833506184\n",
            "Sample 1331: 34.681828101086886\n",
            "Sample 1332: 33.55136565028933\n",
            "Sample 1333: 35.14761993006466\n",
            "Sample 1334: 34.10166784190291\n",
            "Sample 1335: 33.75633681142484\n",
            "Sample 1336: 34.55169212074149\n",
            "Sample 1337: 34.563608765387706\n",
            "Sample 1338: 35.16867755728112\n",
            "Sample 1339: 34.01614946134612\n",
            "Sample 1340: 34.28991909305551\n",
            "Sample 1341: 34.68181015052667\n",
            "Sample 1342: 34.49123939699904\n",
            "Sample 1343: 34.194362914755956\n",
            "Sample 1344: 34.493880251663875\n",
            "Sample 1345: 34.11908422714173\n",
            "Sample 1346: 33.93016504666019\n",
            "Sample 1347: 35.05559816522826\n",
            "Sample 1348: 34.40169950772335\n",
            "Sample 1349: 34.61954156885018\n",
            "Sample 1350: 34.339337390343445\n",
            "Sample 1351: 33.821324514152785\n",
            "Sample 1352: 33.761302247436376\n",
            "Sample 1353: 35.3484363465542\n",
            "Sample 1354: 34.21271298833972\n",
            "Sample 1355: 34.53743969994974\n",
            "Sample 1356: 34.297801981127655\n",
            "Sample 1357: 34.40687960521759\n",
            "Sample 1358: 34.494385945514495\n",
            "Sample 1359: 34.35022126214448\n",
            "Sample 1360: 35.23089024596778\n",
            "Sample 1361: 34.041081169395255\n",
            "Sample 1362: 33.87967212968861\n",
            "Sample 1363: 34.55563848984102\n",
            "Sample 1364: 34.46200277847125\n",
            "Sample 1365: 35.17650484397903\n",
            "Sample 1366: 34.9889097603262\n",
            "Sample 1367: 34.36383322217665\n",
            "Sample 1368: 34.8645836294618\n",
            "Sample 1369: 34.311839554126756\n",
            "Sample 1370: 34.02602401915684\n",
            "Sample 1371: 34.62376847215607\n",
            "Sample 1372: 34.07616864989787\n",
            "Sample 1373: 34.475152276665376\n",
            "Sample 1374: 34.8002455490835\n",
            "Sample 1375: 33.820593919871776\n",
            "Sample 1376: 34.621008524920725\n",
            "Sample 1377: 34.46506611933575\n",
            "Sample 1378: 33.598424242751726\n",
            "Sample 1379: 34.05716937518699\n",
            "Sample 1380: 34.1516691273761\n",
            "Sample 1381: 35.420519704843585\n",
            "Sample 1382: 34.02637885053751\n",
            "Sample 1383: 34.60067258150864\n",
            "Sample 1384: 34.71507467711567\n",
            "Sample 1385: 34.03594973930434\n",
            "Sample 1386: 33.794638900283566\n",
            "Sample 1387: 34.60521617935518\n",
            "Sample 1388: 35.0969518493232\n",
            "Sample 1389: 34.93006283408121\n",
            "Sample 1390: 34.953456755870945\n",
            "Sample 1391: 35.004209599546556\n",
            "Sample 1392: 33.60326084947192\n",
            "Sample 1393: 34.799514339169555\n",
            "Sample 1394: 34.11578521227054\n",
            "Sample 1395: 34.17066969815632\n",
            "Sample 1396: 33.90904238916513\n",
            "Sample 1397: 34.76318548346319\n",
            "Sample 1398: 33.7432605087461\n",
            "Sample 1399: 33.57220352895098\n",
            "Sample 1400: 34.139090742132005\n",
            "Sample 1401: 34.48692820058414\n",
            "Sample 1402: 33.83164090199792\n",
            "Sample 1403: 34.01037761099779\n",
            "Sample 1404: 34.09115947378835\n",
            "Sample 1405: 35.13980353034911\n",
            "Sample 1406: 34.774209816777805\n",
            "Sample 1407: 34.42114712521703\n",
            "Sample 1408: 33.89171125288645\n",
            "Sample 1409: 35.076871652791546\n",
            "Sample 1410: 34.02586346856865\n",
            "Sample 1411: 34.22558951813219\n",
            "Sample 1412: 34.21418146729172\n",
            "Sample 1413: 34.91749595145241\n",
            "Sample 1414: 35.13445549467151\n",
            "Sample 1415: 33.82028824192042\n",
            "Sample 1416: 34.055486429162684\n",
            "Sample 1417: 34.25622579433059\n",
            "Sample 1418: 34.59905255965029\n",
            "Sample 1419: 33.899695039955894\n",
            "Sample 1420: 34.77670406980061\n",
            "Sample 1421: 35.25478969959946\n",
            "Sample 1422: 34.76392605747804\n",
            "Sample 1423: 33.37035874625198\n",
            "Sample 1424: 35.521768770467574\n",
            "Sample 1425: 33.7842767611911\n",
            "Sample 1426: 35.056769228670895\n",
            "Sample 1427: 35.567314655612094\n",
            "Sample 1428: 34.44225431088448\n",
            "Sample 1429: 34.557316316391464\n",
            "Sample 1430: 34.80311238963679\n",
            "Sample 1431: 34.677495438523714\n",
            "Sample 1432: 34.083926596886656\n",
            "Sample 1433: 34.00166472012706\n",
            "Sample 1434: 33.9678304090591\n",
            "Sample 1435: 34.20152100845734\n",
            "Sample 1436: 35.02787550148508\n",
            "Sample 1437: 33.3829296466957\n",
            "Sample 1438: 34.37360613575021\n",
            "Sample 1439: 34.09116057544728\n",
            "Sample 1440: 34.32939077917844\n",
            "Sample 1441: 34.556813279482974\n",
            "Sample 1442: 35.47747015764225\n",
            "Sample 1443: 34.267559499394146\n",
            "Sample 1444: 34.57260586806152\n",
            "Sample 1445: 34.16445912833996\n",
            "Sample 1446: 34.10012905118978\n",
            "Sample 1447: 34.195175355815124\n",
            "Sample 1448: 34.21973288864885\n",
            "Sample 1449: 34.18757280753894\n",
            "Sample 1450: 33.891357296352574\n",
            "Sample 1451: 33.81488415124432\n",
            "Sample 1452: 33.880321460422664\n",
            "Sample 1453: 35.002815871393125\n",
            "Sample 1454: 34.506136353068335\n",
            "Sample 1455: 34.47390288203265\n",
            "Sample 1456: 34.382961520589284\n",
            "Sample 1457: 34.60994777205796\n",
            "Sample 1458: 35.32079941779716\n",
            "Sample 1459: 34.53136217165006\n",
            "Sample 1460: 34.62590504244568\n",
            "Sample 1461: 35.22175891911403\n",
            "Sample 1462: 33.45244782456823\n",
            "Sample 1463: 34.32337790853743\n",
            "Sample 1464: 35.08885387854583\n",
            "Sample 1465: 34.86193942121758\n",
            "Sample 1466: 33.89231476756971\n",
            "Sample 1467: 34.52428931070796\n",
            "Sample 1468: 33.66927944574452\n",
            "Sample 1469: 33.8888770084768\n",
            "Sample 1470: 34.17266068745849\n",
            "Sample 1471: 34.25445951104841\n",
            "Sample 1472: 34.83006706749881\n",
            "Sample 1473: 34.21214297704862\n",
            "Sample 1474: 34.39380132603312\n",
            "Sample 1475: 35.37986641661429\n",
            "Sample 1476: 35.00828560798076\n",
            "Sample 1477: 34.762816816542426\n",
            "Sample 1478: 34.40921106383218\n",
            "Sample 1479: 34.956977074580145\n",
            "Sample 1480: 35.1612192615213\n",
            "Sample 1481: 33.60297273325998\n",
            "Sample 1482: 33.54558382020716\n",
            "Sample 1483: 33.76336209042168\n",
            "Sample 1484: 33.59061672111108\n",
            "Sample 1485: 33.81159168152324\n",
            "Sample 1486: 33.271962523632524\n",
            "Sample 1487: 33.6160552570715\n",
            "Sample 1488: 35.21947842032539\n",
            "Sample 1489: 34.08551859124576\n",
            "Sample 1490: 35.07122176256712\n",
            "Sample 1491: 34.31338209534051\n",
            "Sample 1492: 33.97631237277705\n",
            "Sample 1493: 34.26350759784947\n",
            "Sample 1494: 34.93877254958208\n",
            "Sample 1495: 34.690147051684924\n",
            "Sample 1496: 33.46070773920832\n",
            "Sample 1497: 34.273671438334524\n",
            "Sample 1498: 34.487153392630134\n",
            "Sample 1499: 34.71216108086133\n",
            "Sample 1500: 35.070496222956486\n",
            "Sample 1501: 34.38763909960276\n",
            "Sample 1502: 34.064468643240076\n",
            "Sample 1503: 34.88627953842128\n",
            "Sample 1504: 33.29257546946205\n",
            "Sample 1505: 34.26379947266246\n",
            "Sample 1506: 34.71689047024624\n",
            "Sample 1507: 34.008985114110224\n",
            "Sample 1508: 35.043375972237634\n",
            "Sample 1509: 34.070911339073334\n",
            "Sample 1510: 34.72984280389362\n",
            "Sample 1511: 33.74734455281014\n",
            "Sample 1512: 34.45864223270864\n",
            "Sample 1513: 33.79061583628147\n",
            "Sample 1514: 35.27102581930342\n",
            "Sample 1515: 34.463021586169404\n",
            "Sample 1516: 34.47106000256109\n",
            "Sample 1517: 34.13354871727752\n",
            "Sample 1518: 33.689090189462576\n",
            "Sample 1519: 34.307964201526055\n",
            "Sample 1520: 33.937644500751816\n",
            "Sample 1521: 34.83036555226538\n",
            "Sample 1522: 35.08352787604683\n",
            "Sample 1523: 34.615972129113395\n",
            "Sample 1524: 34.82210567002703\n",
            "Sample 1525: 33.55714164805951\n",
            "Sample 1526: 34.628167525870644\n",
            "Sample 1527: 34.37865144203418\n",
            "Sample 1528: 34.16826646160217\n",
            "Sample 1529: 34.12536235457188\n",
            "Sample 1530: 33.85758587704882\n",
            "Sample 1531: 34.61873693660873\n",
            "Sample 1532: 33.516488878257945\n",
            "Sample 1533: 34.3566654079553\n",
            "Sample 1534: 34.010874135157906\n",
            "Sample 1535: 34.09744012855369\n",
            "Sample 1536: 34.6441804948378\n",
            "Sample 1537: 33.80284295433555\n",
            "Sample 1538: 34.14663273156877\n",
            "Sample 1539: 34.08340227203943\n",
            "Sample 1540: 34.018193913508135\n",
            "Sample 1541: 34.656740767513014\n",
            "Sample 1542: 34.61382583830382\n",
            "Sample 1543: 34.80309466588871\n",
            "Sample 1544: 33.79485780639331\n",
            "Sample 1545: 33.78704775741748\n",
            "Sample 1546: 33.711441812297565\n",
            "Sample 1547: 34.3785884854665\n",
            "Sample 1548: 35.52308298476764\n",
            "Sample 1549: 33.68148744677599\n",
            "Sample 1550: 34.356376870520826\n",
            "Sample 1551: 35.565554982283494\n",
            "Sample 1552: 33.998310168685094\n",
            "Sample 1553: 34.38049236270887\n",
            "Sample 1554: 34.33888890985329\n",
            "Sample 1555: 35.33923911457505\n",
            "Sample 1556: 34.025199816268504\n",
            "Sample 1557: 34.55936161099855\n",
            "Sample 1558: 33.987630946231185\n",
            "Sample 1559: 33.640184763009174\n",
            "Sample 1560: 34.512509466179466\n",
            "Sample 1561: 33.77425458108376\n",
            "Sample 1562: 33.28168161792718\n",
            "Sample 1563: 35.37512785753887\n",
            "Sample 1564: 34.6237025994324\n",
            "Sample 1565: 33.797862483919786\n",
            "Sample 1566: 34.53378211129768\n",
            "Sample 1567: 33.730018049979314\n",
            "Sample 1568: 34.83241599874805\n",
            "Sample 1569: 33.790699691967085\n",
            "Sample 1570: 34.451254588928144\n",
            "Sample 1571: 34.541394655508576\n",
            "Sample 1572: 34.241149818685166\n",
            "Sample 1573: 34.36302413469687\n",
            "Sample 1574: 33.957062535463045\n",
            "Sample 1575: 34.45709432090761\n",
            "Sample 1576: 33.778042991733756\n",
            "Sample 1577: 33.83352253545042\n",
            "Sample 1578: 34.18661404025247\n",
            "Sample 1579: 34.12462826090368\n",
            "Sample 1580: 34.58781382406685\n",
            "Sample 1581: 35.207639280625074\n",
            "Sample 1582: 33.923384983980476\n",
            "Sample 1583: 34.624791816096874\n",
            "Sample 1584: 34.156505507284166\n",
            "Sample 1585: 34.41135247008047\n",
            "Sample 1586: 34.533064655919496\n",
            "Sample 1587: 35.1660444628314\n",
            "Sample 1588: 34.53889147580826\n",
            "Sample 1589: 34.03865784376651\n",
            "Sample 1590: 34.501835411802006\n",
            "Sample 1591: 34.9039638211905\n",
            "Sample 1592: 34.53976975718937\n",
            "Sample 1593: 34.910191499122\n",
            "Sample 1594: 33.93128021711299\n",
            "Sample 1595: 33.61233514927517\n",
            "Sample 1596: 34.88350478359385\n",
            "Sample 1597: 33.46582993480526\n",
            "Sample 1598: 34.201384143536146\n",
            "Sample 1599: 33.60186472359023\n",
            "Sample 1600: 33.97879376231176\n",
            "Sample 1601: 34.39627383749292\n",
            "Sample 1602: 33.659460748521894\n",
            "Sample 1603: 34.07535394071738\n",
            "Sample 1604: 33.2679395244339\n",
            "Sample 1605: 34.69966117261512\n",
            "Sample 1606: 34.81427734647365\n",
            "Sample 1607: 33.8483798966017\n",
            "Sample 1608: 34.09494956932848\n",
            "Sample 1609: 34.16760569305628\n",
            "Sample 1610: 33.74661687468512\n",
            "Sample 1611: 35.00440446357024\n",
            "Sample 1612: 33.94845141843641\n",
            "Sample 1613: 34.22967066224111\n",
            "Sample 1614: 34.735563653913836\n",
            "Sample 1615: 34.74138767105268\n",
            "Sample 1616: 34.47771215939885\n",
            "Sample 1617: 34.961063127551654\n",
            "Sample 1618: 34.85572936982896\n",
            "Sample 1619: 35.04434141428114\n",
            "Sample 1620: 34.01094998761541\n",
            "Sample 1621: 35.04382473624296\n",
            "Sample 1622: 33.84201554815941\n",
            "Sample 1623: 34.22739900912564\n",
            "Sample 1624: 35.02429199939607\n",
            "Sample 1625: 34.52311400263827\n",
            "Sample 1626: 34.22418841858445\n",
            "Sample 1627: 34.75222086654293\n",
            "Sample 1628: 33.24015970403328\n",
            "Sample 1629: 34.210949005580595\n",
            "Sample 1630: 34.215852619085126\n",
            "Sample 1631: 34.863399734932806\n",
            "Sample 1632: 33.81950379595877\n",
            "Sample 1633: 34.12366703108548\n",
            "Sample 1634: 34.853689745525195\n",
            "Sample 1635: 34.13127929988164\n",
            "Sample 1636: 34.66156775091844\n",
            "Sample 1637: 34.375821442251585\n",
            "Sample 1638: 34.949379710581276\n",
            "Sample 1639: 34.67822574118913\n",
            "Sample 1640: 34.45469477815105\n",
            "Sample 1641: 34.535456163046206\n",
            "Sample 1642: 34.58445126939678\n",
            "Sample 1643: 33.933400813348115\n",
            "Sample 1644: 34.18992024830845\n",
            "Sample 1645: 33.911832534815844\n",
            "Sample 1646: 33.87432493645607\n",
            "Sample 1647: 34.42186226387129\n",
            "Sample 1648: 35.02948638605466\n",
            "Sample 1649: 34.06534209676293\n",
            "Sample 1650: 33.67132898497865\n",
            "Sample 1651: 34.14157968127056\n",
            "Sample 1652: 34.30686529674334\n",
            "Sample 1653: 34.628599700188545\n",
            "Sample 1654: 34.05684973208861\n",
            "Sample 1655: 34.828102453207485\n",
            "Sample 1656: 34.331577993377095\n",
            "Sample 1657: 34.20680381944593\n",
            "Sample 1658: 35.064351817479405\n",
            "Sample 1659: 33.64346317037803\n",
            "Sample 1660: 35.30537560954544\n",
            "Sample 1661: 33.480438385841815\n",
            "Sample 1662: 33.475916529558276\n",
            "Sample 1663: 34.493884253277926\n",
            "Sample 1664: 34.43604774268218\n",
            "Sample 1665: 34.62004593422974\n",
            "Sample 1666: 33.708154138033\n",
            "Sample 1667: 34.82742658545391\n",
            "Sample 1668: 34.30877949393774\n",
            "Sample 1669: 35.25311699252287\n",
            "Sample 1670: 34.58802475935022\n",
            "Sample 1671: 34.017750366181865\n",
            "Sample 1672: 33.511740598662556\n",
            "Sample 1673: 34.92554609727153\n",
            "Sample 1674: 34.08148467266303\n",
            "Sample 1675: 33.73156199256803\n",
            "Sample 1676: 34.038654927610516\n",
            "Sample 1677: 33.22114675579097\n",
            "Sample 1678: 34.10848594421744\n",
            "Sample 1679: 34.78855325403822\n",
            "Sample 1680: 33.94925387976174\n",
            "Sample 1681: 34.17485710374939\n",
            "Sample 1682: 34.52706498898478\n",
            "Sample 1683: 34.406526167111444\n",
            "Sample 1684: 34.4445212090456\n",
            "Sample 1685: 34.86084534429312\n",
            "Sample 1686: 34.25021417115631\n",
            "Sample 1687: 34.47311334899888\n",
            "Sample 1688: 34.579915423664914\n",
            "Sample 1689: 33.62863095297174\n",
            "Sample 1690: 33.60106239187183\n",
            "Sample 1691: 34.92670445923473\n",
            "Sample 1692: 34.344440752432945\n",
            "Sample 1693: 34.441095624903944\n",
            "Sample 1694: 34.934247841946025\n",
            "Sample 1695: 34.45952513133673\n",
            "Sample 1696: 35.137176721835644\n",
            "Sample 1697: 33.44682897520424\n",
            "Sample 1698: 34.78079757517076\n",
            "Sample 1699: 33.67542495288053\n",
            "Sample 1700: 34.531739894855264\n",
            "Sample 1701: 35.37146257347517\n",
            "Sample 1702: 34.05174237648549\n",
            "Sample 1703: 33.447038744025214\n",
            "Sample 1704: 33.51093729489216\n",
            "Sample 1705: 35.05609481899531\n",
            "Sample 1706: 34.218493749164686\n",
            "Sample 1707: 34.506447118091806\n",
            "Sample 1708: 33.597569938653216\n",
            "Sample 1709: 34.53656200179983\n",
            "Sample 1710: 33.49521169689708\n",
            "Sample 1711: 34.19349001206257\n",
            "Sample 1712: 34.262638421355405\n",
            "Sample 1713: 34.39014573008766\n",
            "Sample 1714: 34.855794950937025\n",
            "Sample 1715: 34.890157183444614\n",
            "Sample 1716: 33.7882105260164\n",
            "Sample 1717: 34.76347761749005\n",
            "Sample 1718: 34.19349487232255\n",
            "Sample 1719: 35.28424618008813\n",
            "Sample 1720: 35.29902046319539\n",
            "Sample 1721: 34.364778477940355\n",
            "Sample 1722: 34.09924960334628\n",
            "Sample 1723: 35.0616580021816\n",
            "Sample 1724: 34.27171942831743\n",
            "Sample 1725: 34.57180747315371\n",
            "Sample 1726: 34.71206896273373\n",
            "Sample 1727: 33.96446406338623\n",
            "Sample 1728: 33.990413088653796\n",
            "Sample 1729: 34.70469649916526\n",
            "Sample 1730: 34.093715322506036\n",
            "Sample 1731: 34.49313700450603\n",
            "Sample 1732: 34.54344393552884\n",
            "Sample 1733: 34.698848731555955\n",
            "Sample 1734: 34.389695799619936\n",
            "Sample 1735: 33.57343557245556\n",
            "Sample 1736: 34.842660551950594\n",
            "Sample 1737: 33.943201495207056\n",
            "Sample 1738: 34.078880480559356\n",
            "Sample 1739: 33.85700128497777\n",
            "Sample 1740: 34.72289587228774\n",
            "Sample 1741: 34.22865966336067\n",
            "Sample 1742: 33.61465926079675\n",
            "Sample 1743: 34.73993050030557\n",
            "Sample 1744: 34.9976115050007\n",
            "Sample 1745: 34.48774580971976\n",
            "Sample 1746: 34.08118878003512\n",
            "Sample 1747: 34.20241811524546\n",
            "Sample 1748: 34.33692892900982\n",
            "Sample 1749: 33.849412151019145\n",
            "Sample 1750: 34.353507721344045\n",
            "Sample 1751: 34.27274364710503\n",
            "Sample 1752: 34.50432853076415\n",
            "Sample 1753: 33.93089249797308\n",
            "Sample 1754: 34.426534658607444\n",
            "Sample 1755: 34.22099150157468\n",
            "Sample 1756: 34.355835210746314\n",
            "Sample 1757: 34.32252023465867\n",
            "Sample 1758: 34.167757365569564\n",
            "Sample 1759: 34.708946440103475\n",
            "Sample 1760: 35.21664067652854\n",
            "Sample 1761: 35.15244655705102\n",
            "Sample 1762: 33.9004179226244\n",
            "Sample 1763: 33.891955108330784\n",
            "Sample 1764: 34.4426358088916\n",
            "Sample 1765: 33.58623023926901\n",
            "Sample 1766: 35.309910361718806\n",
            "Sample 1767: 34.24011464811173\n",
            "Sample 1768: 33.94895338608771\n",
            "Sample 1769: 34.503961678340445\n",
            "Sample 1770: 35.28016239523795\n",
            "Sample 1771: 34.02784094634807\n",
            "Sample 1772: 34.38451542671097\n",
            "Sample 1773: 33.50705699292671\n",
            "Sample 1774: 34.99235445339003\n",
            "Sample 1775: 34.24246393578004\n",
            "Sample 1776: 34.85120835599049\n",
            "Sample 1777: 33.944503332045024\n",
            "Sample 1778: 34.735534816371256\n",
            "Sample 1779: 33.4806773162227\n",
            "Sample 1780: 34.4982495767882\n",
            "Sample 1781: 33.970464378355615\n",
            "Sample 1782: 33.527745888418934\n",
            "Sample 1783: 34.70812800472366\n",
            "Sample 1784: 34.15890106462753\n",
            "Sample 1785: 34.09116057544728\n",
            "Sample 1786: 34.62831916598219\n",
            "Sample 1787: 34.94536176605303\n",
            "Sample 1788: 34.81421238099851\n",
            "Sample 1789: 34.60958898766583\n",
            "Sample 1790: 34.05072791061959\n",
            "Sample 1791: 34.228153127064985\n",
            "Sample 1792: 35.0259080034395\n",
            "Sample 1793: 34.28457931981989\n",
            "Sample 1794: 35.15698616948437\n",
            "Sample 1795: 33.376714151815904\n",
            "Sample 1796: 34.8693273404128\n",
            "Sample 1797: 34.32594409340802\n",
            "Sample 1798: 34.24635418507759\n",
            "Sample 1799: 35.506927739789845\n",
            "Sample 1800: 34.270550373782264\n",
            "Sample 1801: 34.42173208990801\n",
            "Sample 1802: 33.67717898830755\n",
            "Sample 1803: 34.18238603528766\n",
            "Sample 1804: 34.523340053330195\n",
            "Sample 1805: 34.34042521373339\n",
            "Sample 1806: 34.812004235280185\n",
            "Sample 1807: 33.53681539276566\n",
            "Sample 1808: 34.525830968974475\n",
            "Sample 1809: 34.162870017735116\n",
            "Sample 1810: 33.28445753921701\n",
            "Sample 1811: 34.214193682745154\n",
            "Sample 1812: 34.0775615032045\n",
            "Sample 1813: 34.39985109414785\n",
            "Sample 1814: 34.2785155298551\n",
            "Sample 1815: 34.71361290532245\n",
            "Sample 1816: 34.862962797560115\n",
            "Sample 1817: 33.944427511989254\n",
            "Sample 1818: 34.1346456455545\n",
            "Sample 1819: 34.33480264627052\n",
            "Sample 1820: 34.08450438459373\n",
            "Sample 1821: 33.924036809647994\n",
            "Sample 1822: 34.784673016876226\n",
            "Sample 1823: 34.03865758455264\n",
            "Sample 1824: 34.42083289320811\n",
            "Sample 1825: 33.88630826386928\n",
            "Sample 1826: 34.498696623501665\n",
            "Sample 1827: 34.13793698121492\n",
            "Sample 1828: 34.515065881927484\n",
            "Sample 1829: 34.21974439126414\n",
            "Sample 1830: 35.076713726743755\n",
            "Sample 1831: 34.77379280647105\n",
            "Sample 1832: 33.72445732932476\n",
            "Sample 1833: 34.21687482896527\n",
            "Sample 1834: 34.16565222496119\n",
            "Sample 1835: 34.70462550696774\n",
            "Sample 1836: 34.069870238982716\n",
            "Sample 1837: 34.460233984054746\n",
            "Sample 1838: 34.41206908301359\n",
            "Sample 1839: 34.8812494285502\n",
            "Sample 1840: 34.631749861497255\n",
            "Sample 1841: 33.54141073618018\n",
            "Sample 1842: 34.31995349895861\n",
            "Sample 1843: 34.44723266708715\n",
            "Sample 1844: 33.71065328371753\n",
            "Sample 1845: 33.84588736087076\n",
            "Sample 1846: 33.68409675835352\n",
            "Sample 1847: 33.72256344801709\n",
            "Sample 1848: 34.15979823621911\n",
            "Sample 1849: 33.72476184081371\n",
            "Sample 1850: 34.6565135989613\n",
            "Sample 1851: 34.39247803924774\n",
            "Sample 1852: 34.268935779214225\n",
            "Sample 1853: 33.90925238479823\n",
            "Sample 1854: 33.77525812756556\n",
            "Sample 1855: 34.44787716186252\n",
            "Sample 1856: 33.601143136991055\n",
            "Sample 1857: 35.03381506320464\n",
            "Sample 1858: 33.86878087029239\n",
            "Sample 1859: 34.93936608453151\n",
            "Sample 1860: 34.529976171310004\n",
            "Sample 1861: 34.25796466574646\n",
            "Sample 1862: 34.45908291248153\n",
            "Sample 1863: 34.249130106367424\n",
            "Sample 1864: 34.365925774711656\n",
            "Sample 1865: 34.08047733517844\n",
            "Sample 1866: 34.3657331059054\n",
            "Sample 1867: 34.991623794305546\n",
            "Sample 1868: 34.68240767088928\n",
            "Sample 1869: 34.52421089851352\n",
            "Sample 1870: 34.44335989042423\n",
            "Sample 1871: 34.78563424669442\n",
            "Sample 1872: 34.074554395548034\n",
            "Sample 1873: 34.37501501171393\n",
            "Sample 1874: 34.846367990669236\n",
            "Sample 1875: 33.69231059772894\n",
            "Sample 1876: 35.39691102436596\n",
            "Sample 1877: 33.90028883411919\n",
            "Sample 1878: 33.48994142537925\n",
            "Sample 1879: 33.711746258983055\n",
            "Sample 1880: 35.51240778012865\n",
            "Sample 1881: 34.55550311539956\n",
            "Sample 1882: 34.121645195333265\n",
            "Sample 1883: 34.69095657658811\n",
            "Sample 1884: 34.40338245374765\n",
            "Sample 1885: 34.06979833953667\n",
            "Sample 1886: 33.821090638442286\n",
            "Sample 1887: 33.65808250839695\n",
            "Sample 1888: 34.028485125206544\n",
            "Sample 1889: 34.54335214141858\n",
            "Sample 1890: 33.995166260910906\n",
            "Sample 1891: 34.31271160627465\n",
            "Sample 1892: 34.643568976926424\n",
            "Sample 1893: 34.4987727513739\n",
            "Sample 1894: 34.55850024332219\n",
            "Sample 1895: 33.60963511284458\n",
            "Sample 1896: 33.70048062996103\n",
            "Sample 1897: 34.29175604451788\n",
            "Sample 1898: 34.842660551950594\n",
            "Sample 1899: 34.33779723065709\n",
            "Sample 1900: 35.09937511014848\n",
            "Sample 1901: 34.74079504335178\n",
            "Sample 1902: 34.90792214652959\n",
            "Sample 1903: 33.49493083867338\n",
            "Sample 1904: 33.146803959839545\n",
            "Sample 1905: 34.75191522099331\n",
            "Sample 1906: 34.86582565270022\n",
            "Sample 1907: 34.43473621772597\n",
            "Sample 1908: 33.96213062016548\n",
            "Sample 1909: 34.082812528092795\n",
            "Sample 1910: 34.07521513169219\n",
            "Sample 1911: 34.92254524314957\n",
            "Sample 1912: 34.13455686480543\n",
            "Sample 1913: 35.01012434153846\n",
            "Sample 1914: 34.005536079214146\n",
            "Sample 1915: 35.28864802035176\n",
            "Sample 1916: 34.697102861365714\n",
            "Sample 1917: 35.17131181819324\n",
            "Sample 1918: 34.80359524026548\n",
            "Sample 1919: 35.45744070222485\n",
            "Sample 1920: 33.83652824983236\n",
            "Sample 1921: 33.71846449915575\n",
            "Sample 1922: 35.56409405293533\n",
            "Sample 1923: 35.21517974718038\n",
            "Sample 1924: 34.11090010015568\n",
            "Sample 1925: 34.47931476532559\n",
            "Sample 1926: 33.58848552950919\n",
            "Sample 1927: 34.78818552676772\n",
            "Sample 1928: 34.827584543903434\n",
            "Sample 1929: 33.626153192431154\n",
            "Sample 1930: 34.357682141942526\n",
            "Sample 1931: 35.13623033201127\n",
            "Sample 1932: 34.983649857363005\n",
            "Sample 1933: 34.03134833676571\n",
            "Sample 1934: 33.493525769913326\n",
            "Sample 1935: 34.75696655399966\n",
            "Sample 1936: 34.524413004324586\n",
            "Sample 1937: 34.35995923854918\n",
            "Sample 1938: 34.27808196226266\n",
            "Sample 1939: 34.515220956622755\n",
            "Sample 1940: 34.29897307697202\n",
            "Sample 1941: 33.71825473033478\n",
            "Sample 1942: 34.434876695440416\n",
            "Sample 1943: 35.24821052766582\n",
            "Sample 1944: 34.97451963216819\n",
            "Sample 1945: 34.80827168521829\n",
            "Sample 1946: 34.78211687654294\n",
            "Sample 1947: 33.79967043583091\n",
            "Sample 1948: 34.148908337695694\n",
            "Sample 1949: 34.21673982714373\n",
            "Sample 1950: 34.17513614747602\n",
            "Sample 1951: 34.40278865958436\n",
            "Sample 1952: 33.575922470284915\n",
            "Sample 1953: 34.14473220790579\n",
            "Sample 1954: 34.1253774861813\n",
            "Sample 1955: 34.962823060094124\n",
            "Sample 1956: 34.633062585317596\n",
            "Sample 1957: 34.626687155482536\n",
            "Sample 1958: 35.048216596772754\n",
            "Sample 1959: 34.24335819121563\n",
            "Sample 1960: 34.77282247176582\n",
            "Sample 1961: 34.52091047416693\n",
            "Sample 1962: 34.32039999484261\n",
            "Sample 1963: 34.5369952129732\n",
            "Sample 1964: 34.080399247001345\n",
            "Sample 1965: 33.90385312198039\n",
            "Sample 1966: 33.84289272788158\n",
            "Sample 1967: 33.752606723894665\n",
            "Sample 1968: 34.50372414123409\n",
            "Sample 1969: 35.33118812631088\n",
            "Sample 1970: 34.579187939950295\n",
            "Sample 1971: 34.38216796164016\n",
            "Sample 1972: 35.183953678432744\n",
            "Sample 1973: 35.07700449989782\n",
            "Sample 1974: 34.35058499590136\n",
            "Sample 1975: 34.99529918771002\n",
            "Sample 1976: 34.63042197366576\n",
            "Sample 1977: 34.71390588179437\n",
            "Sample 1978: 34.267621386704626\n",
            "Sample 1979: 34.66038755018704\n",
            "Sample 1980: 34.45372975733007\n",
            "Sample 1981: 33.79485897285571\n",
            "Sample 1982: 34.5451841678175\n",
            "Sample 1983: 33.73222130303593\n",
            "Sample 1984: 33.24856873144972\n",
            "Sample 1985: 34.27880742086896\n",
            "Sample 1986: 34.08026643229681\n",
            "Sample 1987: 33.482140254478324\n",
            "Sample 1988: 33.865245452375504\n",
            "Sample 1989: 34.62961844308323\n",
            "Sample 1990: 34.165990790671785\n",
            "Sample 1991: 34.649863402428394\n",
            "Sample 1992: 34.80711173557016\n",
            "Sample 1993: 34.38239149309732\n",
            "Sample 1994: 34.009499264813215\n",
            "Sample 1995: 34.78298222963249\n",
            "Sample 1996: 34.637516365559605\n",
            "Sample 1997: 33.31780021878657\n",
            "Sample 1998: 34.20811903819973\n",
            "Sample 1999: 34.58618784028958\n",
            "Sample 2000: 34.49869314031534\n",
            "Test Loss: 1.012622356414795\n"
          ]
        }
      ]
    }
  ]
}